# Task08：Decoder-only PLM 学习笔记


### Decoder-only模型的本质

Decoder-only预训练语言模型是自回归生成的体现，以GPT系列为典型代表。这种架构的核心是：**通过预测下一个词来学习语言的本质规律**。

通过单向注意力，只能看到当前位置之前的信息。因果编码严格的做时序约束。通过自回归生成来逐步的生成序列。把理解和生成使用相同机制来统一框架。

三种架构的注意力模式对比：
```python
# 三种架构的注意力模式对比
class AttentionComparison:
    def encoder_only_attention(self, seq_len):
        # BERT: 双向注意力，所有位置互相可见
        mask = torch.zeros(seq_len, seq_len)  # 全0，无掩码
        return mask
    
    def encoder_decoder_attention(self, src_len, tgt_len):
        # T5: Encoder双向 + Decoder单向 + 交叉注意力
        encoder_mask = torch.zeros(src_len, src_len)
        decoder_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1)
        cross_mask = torch.zeros(tgt_len, src_len)
        return encoder_mask, decoder_mask, cross_mask
    
    def decoder_only_attention(self, seq_len):
        # GPT: 纯因果注意力
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
        return mask.bool()  # 上三角为True（被掩码）
```

### Transformer Decoder的核心组件

**1. 多头自注意力（带因果掩码）**
```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_heads = config.num_heads
        self.head_dim = config.hidden_size // config.num_heads
        
        # 合并的QKV投影
        self.qkv_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size)
        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)
        
        # 因果掩码（注册为buffer，不参与梯度更新）
        self.register_buffer(
            "causal_mask",
            torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool()
        )
    
    def forward(self, x):
        B, T, C = x.shape
        
        # 计算QKV
        qkv = self.qkv_proj(x)  # [B, T, 3*C]
        q, k, v = qkv.chunk(3, dim=-1)  # 各自 [B, T, C]
        
        # 重塑为多头
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, D]
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 计算注意力分数
        scores = q @ k.transpose(-2, -1) / math.sqrt(self.head_dim)  # [B, H, T, T]
        
        # 应用因果掩码
        scores = scores.masked_fill(self.causal_mask[:T, :T], float('-inf'))
        
        # Softmax和加权求和
        attn_weights = F.softmax(scores, dim=-1)
        out = attn_weights @ v  # [B, H, T, D]
        
        # 合并多头
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.out_proj(out)
```

**2. 位置编码的演进**
```python
class PositionalEncoding:
    def __init__(self):
        pass
    
    def absolute_positional_encoding(self, seq_len, d_model):
        """GPT-1/2使用的绝对位置编码"""
        pe = torch.zeros(seq_len, d_model)
        position = torch.arange(0, seq_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe
    
    def learned_positional_encoding(self, max_seq_len, d_model):
        """GPT-2/3使用的可学习位置编码"""
        return nn.Embedding(max_seq_len, d_model)
    
    def rotary_positional_encoding(self, seq_len, d_model):
        """现代模型使用的旋转位置编码（RoPE）"""
        # RoPE的核心思想：通过旋转矩阵编码相对位置
        def rotate_half(x):
            x1, x2 = x.chunk(2, dim=-1)
            return torch.cat((-x2, x1), dim=-1)
        
        def apply_rope(q, k, cos, sin):
            q_rot = q * cos + rotate_half(q) * sin
            k_rot = k * cos + rotate_half(k) * sin
            return q_rot, k_rot
        
        return apply_rope
```

**3. 前馈网络的设计哲学**
```python
class GPTFeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        # 通常是4倍的隐藏层大小
        self.fc1 = nn.Linear(config.hidden_size, 4 * config.hidden_size)
        self.fc2 = nn.Linear(4 * config.hidden_size, config.hidden_size)
        self.activation = nn.GELU()  # GPT使用GELU而非ReLU
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x):
        # 两层线性变换 + 激活函数
        x = self.fc1(x)
        x = self.activation(x)  # GELU提供更平滑的梯度
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

### 自回归语言建模

**核心思想**：给定前面的词，预测下一个词

```python
class AutoregressiveLanguageModeling:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def compute_loss(self, input_ids):
        """计算自回归语言建模损失"""
        # 输入：[BOS, w1, w2, w3, w4]
        # 目标：[w1, w2, w3, w4, EOS]
        
        inputs = input_ids[:, :-1]  # 去掉最后一个token作为输入
        targets = input_ids[:, 1:]  # 去掉第一个token作为目标
        
        # 前向传播
        logits = self.model(inputs)  # [batch_size, seq_len, vocab_size]
        
        # 计算交叉熵损失
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),  # [batch_size * seq_len, vocab_size]
            targets.view(-1),                   # [batch_size * seq_len]
            ignore_index=self.tokenizer.pad_token_id
        )
        
        return loss
    
    def perplexity(self, text_dataset):
        """计算困惑度 - 语言模型质量的重要指标"""
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in text_dataset:
                loss = self.compute_loss(batch['input_ids'])
                total_loss += loss.item() * batch['input_ids'].numel()
                total_tokens += batch['input_ids'].numel()
        
        avg_loss = total_loss / total_tokens
        perplexity = math.exp(avg_loss)
        return perplexity
```

### 数据预处理

```python
class GPTDataPreprocessor:
    def __init__(self, tokenizer, max_length=1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def prepare_training_data(self, texts):
        """为GPT训练准备数据"""
        processed_data = []
        
        for text in texts:
            # 1. 分词
            tokens = self.tokenizer.encode(text)
            
            # 2. 添加特殊token
            tokens = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]
            
            # 3. 截断或填充
            if len(tokens) > self.max_length:
                # 滑动窗口处理长文本
                for i in range(0, len(tokens) - self.max_length + 1, self.max_length // 2):
                    chunk = tokens[i:i + self.max_length]
                    processed_data.append(chunk)
            else:
                # 填充短文本
                tokens += [self.tokenizer.pad_token_id] * (self.max_length - len(tokens))
                processed_data.append(tokens)
        
        return processed_data
    
    def create_attention_mask(self, input_ids):
        """创建注意力掩码"""
        # 1表示真实token，0表示padding
        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()
        return attention_mask
```

### 贪心解码 vs 采样策略

```python
class GenerationStrategies:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def greedy_decode(self, prompt, max_length=100):
        """贪心解码：每步选择概率最高的词"""
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        for _ in range(max_length):
            with torch.no_grad():
                outputs = self.model(input_ids)
                logits = outputs.logits[:, -1, :]  # 最后一个位置的logits
                
                # 选择概率最高的token
                next_token = torch.argmax(logits, dim=-1, keepdim=True)
                
                # 添加到序列
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                
                # 检查结束条件
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
    
    def nucleus_sampling(self, prompt, max_length=100, top_p=0.9, temperature=1.0):
        """核采样：从累积概率为top_p的最小集合中采样"""
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        for _ in range(max_length):
            with torch.no_grad():
                outputs = self.model(input_ids)
                logits = outputs.logits[:, -1, :] / temperature
                
                # 计算概率
                probs = F.softmax(logits, dim=-1)
                
                # 排序并计算累积概率
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                # 找到累积概率超过top_p的位置
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                # 移除低概率token
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = float('-inf')
                
                # 重新计算概率并采样
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
    
    def contrastive_search(self, prompt, max_length=100, alpha=0.6, k=4):
        """对比搜索：平衡流畅性和多样性"""
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        for step in range(max_length):
            with torch.no_grad():
                outputs = self.model(input_ids)
                logits = outputs.logits[:, -1, :]
                
                # 选择top-k候选
                top_k_probs, top_k_indices = torch.topk(F.softmax(logits, dim=-1), k)
                
                if step == 0:
                    # 第一步直接选择概率最高的
                    next_token = top_k_indices[:, 0:1]
                else:
                    # 计算对比分数
                    scores = []
                    for i in range(k):
                        candidate_input = torch.cat([input_ids, top_k_indices[:, i:i+1]], dim=-1)
                        
                        # 计算与历史的相似度（惩罚重复）
                        candidate_hidden = self.model(candidate_input, output_hidden_states=True).hidden_states[-1]
                        history_hidden = self.model(input_ids, output_hidden_states=True).hidden_states[-1]
                        
                        similarity = torch.cosine_similarity(
                            candidate_hidden[:, -1:, :],
                            history_hidden.mean(dim=1, keepdim=True),
                            dim=-1
                        )
                        
                        # 对比分数 = α * 概率 - (1-α) * 相似度
                        score = alpha * top_k_probs[:, i] - (1 - alpha) * similarity
                        scores.append(score)
                    
                    # 选择分数最高的候选
                    best_idx = torch.argmax(torch.stack(scores), dim=0)
                    next_token = top_k_indices[:, best_idx:best_idx+1]
                
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
```

### 参数规模的演进

```python
class ModelScaling:
    def __init__(self):
        self.gpt_evolution = {
            'GPT-1': {
                'parameters': '117M',
                'layers': 12,
                'hidden_size': 768,
                'heads': 12,
                'context_length': 512,
                'key_innovations': ['Transformer decoder', 'Unsupervised pre-training']
            },
            'GPT-2': {
                'parameters': '1.5B',
                'layers': 48,
                'hidden_size': 1600,
                'heads': 25,
                'context_length': 1024,
                'key_innovations': ['Zero-shot task transfer', 'Larger scale']
            },
            'GPT-3': {
                'parameters': '175B',
                'layers': 96,
                'hidden_size': 12288,
                'heads': 96,
                'context_length': 2048,
                'key_innovations': ['In-context learning', 'Few-shot prompting']
            },
            'GPT-4': {
                'parameters': '~1.7T (estimated)',
                'architecture': 'Mixture of Experts',
                'context_length': 32768,
                'key_innovations': ['Multimodal', 'Advanced reasoning', 'Tool use']
            }
        }
    
    def scaling_laws(self, N, D, C):
        """Kaplan等人的缩放定律
        N: 参数数量
        D: 数据集大小
        C: 计算量
        """
        # 损失与参数数量的关系
        loss_N = 1.73 * (N / 8.8e6) ** (-0.076)
        
        # 损失与数据集大小的关系
        loss_D = 5.4 * (D / 5.4e9) ** (-0.095)
        
        # 损失与计算量的关系
        loss_C = 1.6 * (C / 3.2e8) ** (-0.050)
        
        return {
            'loss_from_params': loss_N,
            'loss_from_data': loss_D,
            'loss_from_compute': loss_C
        }
```

### 涌现能力

```python
class EmergentAbilities:
    def __init__(self):
        self.abilities = {
            'in_context_learning': {
                'description': '通过示例学习新任务，无需参数更新',
                'emergence_scale': '~1B parameters',
                'example': self.few_shot_example
            },
            'chain_of_thought': {
                'description': '逐步推理解决复杂问题',
                'emergence_scale': '~100B parameters',
                'example': self.cot_example
            },
            'instruction_following': {
                'description': '理解和执行自然语言指令',
                'emergence_scale': '~10B parameters',
                'example': self.instruction_example
            }
        }
    
    def few_shot_example(self):
        prompt = """
        Translate English to French:
        
        English: Hello
        French: Bonjour
        
        English: Thank you
        French: Merci
        
        English: Good morning
        French:
        """
        return prompt
    
    def cot_example(self):
        prompt = """
        Question: A store has 20 apples. They sell 8 apples in the morning and 5 apples in the afternoon. How many apples are left?
        
        Let me think step by step:
        1. The store starts with 20 apples
        2. They sell 8 apples in the morning: 20 - 8 = 12 apples left
        3. They sell 5 apples in the afternoon: 12 - 5 = 7 apples left
        
        Therefore, there are 7 apples left.
        """
        return prompt
    
    def instruction_example(self):
        prompt = """
        Please write a short poem about artificial intelligence in the style of Shakespeare.
        """
        return prompt
```

### 指令微调

```python
class InstructionTuning:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def prepare_instruction_data(self, instruction_dataset):
        """准备指令微调数据"""
        formatted_data = []
        
        for sample in instruction_dataset:
            # 格式化指令数据
            if 'input' in sample and sample['input']:
                prompt = f"Instruction: {sample['instruction']}\nInput: {sample['input']}\nResponse: "
            else:
                prompt = f"Instruction: {sample['instruction']}\nResponse: "
            
            # 完整的训练样本
            full_text = prompt + sample['output'] + self.tokenizer.eos_token
            
            # 分词
            tokenized = self.tokenizer(
                full_text,
                truncation=True,
                padding=False,
                return_tensors='pt'
            )
            
            # 计算损失掩码（只对response部分计算损失）
            prompt_length = len(self.tokenizer.encode(prompt))
            labels = tokenized['input_ids'].clone()
            labels[:, :prompt_length] = -100  # 忽略prompt部分的损失
            
            formatted_data.append({
                'input_ids': tokenized['input_ids'],
                'attention_mask': tokenized['attention_mask'],
                'labels': labels
            })
        
        return formatted_data
    
    def instruction_tuning_loss(self, batch):
        """计算指令微调损失"""
        outputs = self.model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels']
        )
        return outputs.loss
```

### 人类反馈强化学习（RLHF）

```python
class RLHF:
    def __init__(self, policy_model, value_model, reward_model):
        self.policy_model = policy_model  # 要训练的GPT模型
        self.value_model = value_model    # 价值函数
        self.reward_model = reward_model  # 奖励模型
    
    def ppo_step(self, prompts, responses, rewards):
        """PPO算法的一步更新"""
        # 计算旧策略的概率
        with torch.no_grad():
            old_log_probs = self.compute_log_probs(prompts, responses)
            old_values = self.value_model(prompts, responses)
        
        # 计算优势函数
        advantages = self.compute_advantages(rewards, old_values)
        
        # 多轮更新
        for _ in range(self.ppo_epochs):
            # 当前策略的概率
            new_log_probs = self.compute_log_probs(prompts, responses)
            new_values = self.value_model(prompts, responses)
            
            # 计算比率
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # PPO裁剪目标
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # 价值函数损失
            value_loss = F.mse_loss(new_values, rewards)
            
            # 总损失
            total_loss = policy_loss + self.value_coef * value_loss
            
            # 反向传播
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
    
    def compute_advantages(self, rewards, values):
        """计算GAE优势函数"""
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value - values[t]
            gae = delta + self.gamma * self.gae_lambda * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages)
```

### 高效推理优化

```python
class EfficientInference:
    def __init__(self, model):
        self.model = model
        self.kv_cache = {}  # KV缓存
    
    def generate_with_kv_cache(self, input_ids, max_length=100):
        """使用KV缓存的高效生成"""
        batch_size, seq_len = input_ids.shape
        
        # 初始化KV缓存
        past_key_values = None
        
        for step in range(max_length):
            if step == 0:
                # 第一步：处理完整输入
                outputs = self.model(
                    input_ids,
                    use_cache=True,
                    return_dict=True
                )
            else:
                # 后续步骤：只处理新token
                outputs = self.model(
                    input_ids[:, -1:],  # 只输入最新的token
                    past_key_values=past_key_values,
                    use_cache=True,
                    return_dict=True
                )
            
            # 更新KV缓存
            past_key_values = outputs.past_key_values
            
            # 生成下一个token
            logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(logits, dim=-1, keepdim=True)
            
            # 添加到序列
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            # 检查结束条件
            if next_token.item() == self.tokenizer.eos_token_id:
                break
        
        return input_ids
    
    def batch_generation(self, prompts, max_length=100):
        """批量生成优化"""
        # 动态批处理：根据生成长度动态调整批大小
        batch_size = len(prompts)
        active_sequences = list(range(batch_size))
        
        # 分词并填充
        input_ids = self.tokenizer(
            prompts,
            padding=True,
            return_tensors='pt'
        )['input_ids']
        
        finished_sequences = []
        
        for step in range(max_length):
            if not active_sequences:
                break
            
            # 只处理未完成的序列
            active_input_ids = input_ids[active_sequences]
            
            with torch.no_grad():
                outputs = self.model(active_input_ids)
                logits = outputs.logits[:, -1, :]
                next_tokens = torch.argmax(logits, dim=-1, keepdim=True)
            
            # 更新序列
            for i, seq_idx in enumerate(active_sequences):
                input_ids[seq_idx] = torch.cat([
                    input_ids[seq_idx],
                    next_tokens[i]
                ], dim=-1)
                
                # 检查是否完成
                if next_tokens[i].item() == self.tokenizer.eos_token_id:
                    finished_sequences.append(seq_idx)
            
            # 移除已完成的序列
            active_sequences = [idx for idx in active_sequences if idx not in finished_sequences]
        
        return [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in input_ids]
```

### 内存优化

```python
class MemoryOptimization:
    def __init__(self):
        pass
    
    def gradient_checkpointing(self, model):
        """梯度检查点：用计算换内存"""
        def checkpoint_forward(module, input):
            def custom_forward(*inputs):
                return module(*inputs)
            return torch.utils.checkpoint.checkpoint(custom_forward, input)
        
        # 为每个Transformer层启用梯度检查点
        for layer in model.transformer.layers:
            layer.forward = lambda x: checkpoint_forward(layer, x)
    
    def mixed_precision_training(self, model, optimizer):
        """混合精度训练"""
        from torch.cuda.amp import GradScaler, autocast
        
        scaler = GradScaler()
        
        def training_step(batch):
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(**batch)
                loss = outputs.loss
            
            # 缩放损失并反向传播
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            return loss
        
        return training_step
    
    def model_parallelism(self, model, device_map):
        """模型并行：将不同层放在不同GPU上"""
        from accelerate import dispatch_model
        
        # 自动分配模型到多个GPU
        model = dispatch_model(model, device_map=device_map)
        return model
```

### 语言模型评估指标

```python
class LanguageModelEvaluation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def perplexity_evaluation(self, test_texts):
        """困惑度评估"""
        total_loss = 0
        total_tokens = 0
        
        self.model.eval()
        with torch.no_grad():
            for text in test_texts:
                input_ids = self.tokenizer.encode(text, return_tensors='pt')
                
                # 计算损失
                outputs = self.model(input_ids, labels=input_ids)
                loss = outputs.loss
                
                total_loss += loss.item() * input_ids.numel()
                total_tokens += input_ids.numel()
        
        avg_loss = total_loss / total_tokens
        perplexity = math.exp(avg_loss)
        return perplexity
    
    def downstream_task_evaluation(self, task_name, test_data):
        """下游任务评估"""
        if task_name == 'text_classification':
            return self.text_classification_eval(test_data)
        elif task_name == 'question_answering':
            return self.qa_eval(test_data)
        elif task_name == 'text_generation':
            return self.generation_eval(test_data)
    
    def text_classification_eval(self, test_data):
        """文本分类评估"""
        predictions = []
        labels = []
        
        for sample in test_data:
            # 构造prompt
            prompt = f"Classify the sentiment of this text: {sample['text']}\nSentiment:"
            
            # 生成预测
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + 10,
                    do_sample=False
                )
            
            prediction = self.tokenizer.decode(
                outputs[0][input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            predictions.append(prediction)
            labels.append(sample['label'])
        
        # 计算准确率
        accuracy = sum(p.lower() == l.lower() for p, l in zip(predictions, labels)) / len(labels)
        return {'accuracy': accuracy}
```


### 架构创新

```python
class ArchitecturalInnovations:
    def __init__(self):
        self.innovations = {
            'mixture_of_experts': self.moe_explanation,
            'retrieval_augmented': self.rag_explanation,
            'multimodal_integration': self.multimodal_explanation,
            'efficient_attention': self.efficient_attention_explanation
        }
    
    def moe_explanation(self):
        """专家混合模型"""
        return """
        MoE通过条件计算提高模型容量而不增加计算成本：
        - 每个token只激活部分专家
        - 路由网络决定使用哪些专家
        - 实现参数规模和计算效率的解耦
        """
    
    def rag_explanation(self):
        """检索增强生成"""
        return """
        RAG结合参数化知识和非参数化知识：
        - 检索相关文档作为上下文
        - 生成时考虑检索到的信息
        - 解决知识更新和幻觉问题
        """
    
    def multimodal_explanation(self):
        """多模态集成"""
        return """
        将视觉、听觉等模态集成到语言模型：
        - 统一的多模态表示
        - 跨模态注意力机制
        - 多模态指令跟随能力
        """
    
    def efficient_attention_explanation(self):
        """高效注意力机制"""
        return """
        解决注意力机制的二次复杂度问题：
        - 线性注意力（Linear Attention）
        - 稀疏注意力（Sparse Attention）
        - 滑动窗口注意力（Sliding Window）
        """
```

### 能力边界与局限性

```python
class ModelLimitations:
    def __init__(self):
        self.limitations = {
            'hallucination': {
                'description': '生成看似合理但实际错误的信息',
                'causes': ['训练数据噪声', '过度泛化', '缺乏事实验证'],
                'mitigation': ['检索增强', '事实检查', '不确定性量化']
            },
            'context_length': {
                'description': '有限的上下文窗口',
                'causes': ['注意力复杂度', '内存限制', '位置编码'],
                'mitigation': ['高效注意力', '层次化处理', '外部记忆']
            },
            'reasoning_consistency': {
                'description': '推理过程不一致',
                'causes': ['训练目标局限', '缺乏符号推理', '概率性生成'],
                'mitigation': ['思维链训练', '工具使用', '验证机制']
            },
            'bias_and_fairness': {
                'description': '训练数据中的偏见',
                'causes': ['数据偏见', '社会偏见', '标注偏见'],
                'mitigation': ['数据去偏', '公平性约束', '多样性增强']
            }
        }
    
    def analyze_limitation(self, limitation_type):
        """分析特定局限性"""
        if limitation_type in self.limitations:
            return self.limitations[limitation_type]
        else:
            return "未知的局限性类型"
```

## 思考

### 理论
1. **为什么Decoder-only架构能够同时处理理解和生成任务？**
   - 自回归建模的本质是什么？
   - 因果注意力如何影响表示学习？

2. **涌现能力的机制是什么？**
   - 为什么某些能力只在大规模模型中出现？
   - 如何预测和控制涌现能力？

3. **上下文学习的工作原理？**
   - 模型如何从示例中学习新任务？
   - 与传统机器学习的区别是什么？