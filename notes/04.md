# Task05ï¼šæ­å»ºä¸€ä¸ª Transformer -

**é—®é¢˜ä¸€ï¼šTransformerçš„æœ€å°å¯è¿è¡Œå•å…ƒæ˜¯ä»€ä¹ˆï¼Ÿ**

åœ¨å¼€å§‹ç¼–ç ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®ï¼š**ä¸€ä¸ªèƒ½å¤Ÿå·¥ä½œçš„Transformeræœ€å°‘éœ€è¦å“ªäº›ç»„ä»¶ï¼Ÿ**

**æ ¸å¿ƒç»„ä»¶æ¸…å•ï¼š**
```python
# æœ€å°Transformeræ¶æ„
class MinimalTransformer:
    def __init__(self):
        # 1. åµŒå…¥å±‚ï¼šå°†tokenè½¬æ¢ä¸ºå‘é‡
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # 2. ä½ç½®ç¼–ç ï¼šæ³¨å…¥ä½ç½®ä¿¡æ¯
        self.positional_encoding = PositionalEncoding(d_model)
        
        # 3. ç¼–ç å™¨å±‚ï¼šç†è§£è¾“å…¥åºåˆ—
        self.encoder = TransformerEncoder(num_layers=6)
        
        # 4. è§£ç å™¨å±‚ï¼šç”Ÿæˆè¾“å‡ºåºåˆ—
        self.decoder = TransformerDecoder(num_layers=6)
        
        # 5. è¾“å‡ºæŠ•å½±ï¼šè½¬æ¢ä¸ºè¯æ±‡è¡¨æ¦‚ç‡
        self.output_projection = nn.Linear(d_model, vocab_size)
```

ä¸ºä»€ä¹ˆè¿™äº”ä¸ªç»„ä»¶ç¼ºä¸€ä¸å¯ï¼Ÿæ¯ä¸ªç»„ä»¶è§£å†³äº†ä»€ä¹ˆæ ¸å¿ƒé—®é¢˜ï¼Ÿ

**é—®é¢˜äºŒï¼šå¦‚ä½•å¤„ç†åºåˆ—çš„åŠ¨æ€é•¿åº¦ï¼Ÿ**

ç°å®ä¸­çš„æ–‡æœ¬åºåˆ—é•¿åº¦å„ä¸ç›¸åŒï¼Œä½†ç¥ç»ç½‘ç»œéœ€è¦å›ºå®šçš„è¾“å…¥ç»´åº¦ã€‚**Transformerå¦‚ä½•ä¼˜é›…åœ°è§£å†³è¿™ä¸ªçŸ›ç›¾ï¼Ÿ**


1. **Paddingç­–ç•¥**ï¼š
```python
def pad_sequences(sequences, max_length, pad_token=0):
    """
    å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
    """
    padded = []
    for seq in sequences:
        if len(seq) < max_length:
            # å³å¡«å……ï¼š[1,2,3] -> [1,2,3,0,0]
            padded_seq = seq + [pad_token] * (max_length - len(seq))
        else:
            # æˆªæ–­ï¼šä¿ç•™å‰max_lengthä¸ªtoken
            padded_seq = seq[:max_length]
        padded.append(padded_seq)
    return padded
```

2. **æ³¨æ„åŠ›æ©ç **ï¼š
```python
def create_padding_mask(sequences, pad_token=0):
    """
    åˆ›å»ºå¡«å……æ©ç ï¼Œé˜²æ­¢æ¨¡å‹å…³æ³¨å¡«å……ä½ç½®
    """
    # Trueè¡¨ç¤ºéœ€è¦æ©ç çš„ä½ç½®ï¼ˆå¡«å……ä½ç½®ï¼‰
    mask = (sequences == pad_token)
    return mask
```

é€šè¿‡å¡«å……è§£å†³äº†ç»´åº¦é—®é¢˜ï¼Œæ©ç è§£å†³äº†è¯­ä¹‰é—®é¢˜ã€‚

**é—®é¢˜ä¸‰ï¼šå¦‚ä½•å®ç°é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼Ÿ**

æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformerçš„æ ¸å¿ƒï¼Œä½†å…¶O(nÂ²)çš„å¤æ‚åº¦å¦‚ä½•åœ¨å®é™…ä¸­ä¼˜åŒ–ï¼Ÿ

**å¤šå¤´æ³¨æ„åŠ›çš„å®Œæ•´å®ç°ï¼š**

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # çº¿æ€§å˜æ¢å±‚
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
    
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len = query.size(0), query.size(1)
        
        # 1. çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 2. è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 3. é‡å¡‘å¹¶é€šè¿‡è¾“å‡ºæŠ•å½±
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        return self.W_o(attention_output)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            scores.masked_fill_(mask == 0, -1e9)
        
        # Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åŠ æƒæ±‚å’Œ
        return torch.matmul(attention_weights, V)
```

**æ€§èƒ½ä¼˜åŒ–æ€è€ƒï¼š**
- **ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ** é˜²æ­¢softmaxæ¢¯åº¦æ¶ˆå¤±
- **ä¸ºä»€ä¹ˆè¦å¤šå¤´ï¼Ÿ** æ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»
- **dropoutçš„ä½œç”¨ï¼Ÿ** é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

**é—®é¢˜å››ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŠ€å·§æœ‰å“ªäº›ï¼Ÿ**

ç†è®ºä¸Šå®Œç¾çš„æ¶æ„åœ¨å®é™…è®­ç»ƒä¸­å¯èƒ½é¢ä¸´å„ç§æŒ‘æˆ˜ã€‚**å¦‚ä½•ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Ÿ**


1. **å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
class TransformerLRScheduler:
    def __init__(self, d_model, warmup_steps=4000):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
    
    def get_lr(self, step):
        # TransformeråŸè®ºæ–‡çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
        arg1 = step ** (-0.5)
        arg2 = step * (self.warmup_steps ** (-1.5))
        return (self.d_model ** (-0.5)) * min(arg1, arg2)
```

2. **æ¢¯åº¦è£å‰ª**ï¼š
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

3. **æ ‡ç­¾å¹³æ»‘**ï¼š
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, vocab_size, smoothing=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
    
    def forward(self, pred, target):
        # å°†ç¡¬æ ‡ç­¾è½¬æ¢ä¸ºè½¯æ ‡ç­¾ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (self.vocab_size - 1))
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        
        return F.kl_div(F.log_softmax(pred, dim=1), true_dist, reduction='batchmean')
```

## å®è·µä¸­çš„å·¥ç¨‹æŒ‘æˆ˜

### ğŸ’¾ å†…å­˜ç®¡ç†ï¼šå¦‚ä½•å¤„ç†å¤§æ¨¡å‹ï¼Ÿ

**æŒ‘æˆ˜ï¼š** ç°ä»£Transformeræ¨¡å‹å‚æ•°é‡å·¨å¤§ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„GPUå†…å­˜ä¸­è®­ç»ƒï¼Ÿ

**è§£å†³ç­–ç•¥ï¼š**

1. **æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
# æ¨¡æ‹Ÿå¤§batch_sizeè®­ç»ƒ
accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

2. **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### ğŸ”„ æ•°æ®æµæ°´çº¿ï¼šå¦‚ä½•é«˜æ•ˆåŠ è½½æ•°æ®ï¼Ÿ

**æ ¸å¿ƒæ€æƒ³ï¼š** è®©GPUè®¡ç®—å’Œæ•°æ®åŠ è½½å¹¶è¡Œè¿›è¡Œï¼Œé¿å…GPUç©ºé—²ã€‚

```python
class TransformerDataLoader:
    def __init__(self, dataset, batch_size, num_workers=4):
        self.dataloader = DataLoader(
            dataset, 
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,  # åŠ é€ŸGPUä¼ è¾“
            prefetch_factor=2  # é¢„å–æ•°æ®
        )
    
    def __iter__(self):
        for batch in self.dataloader:
            # å¼‚æ­¥ä¼ è¾“åˆ°GPU
            yield {k: v.cuda(non_blocking=True) for k, v in batch.items()}
```

## è°ƒè¯•ä¸ä¼˜åŒ–çš„å®æˆ˜ç»éªŒ

### ğŸ› å¸¸è§é—®é¢˜è¯Šæ–­

**é—®é¢˜1ï¼šè®­ç»ƒä¸æ”¶æ•›**
- **ç—‡çŠ¶ï¼š** æŸå¤±å‡½æ•°ä¸ä¸‹é™æˆ–éœ‡è¡
- **å¯èƒ½åŸå› ï¼š** å­¦ä¹ ç‡è¿‡å¤§ã€æ¢¯åº¦çˆ†ç‚¸ã€æ•°æ®é¢„å¤„ç†é—®é¢˜
- **è¯Šæ–­æ–¹æ³•ï¼š**
```python
# ç›‘æ§æ¢¯åº¦èŒƒæ•°
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: {grad_norm}")
```

**é—®é¢˜2ï¼šæ¨ç†é€Ÿåº¦æ…¢**
- **ç—‡çŠ¶ï¼š** ç”Ÿæˆæ–‡æœ¬è€—æ—¶è¿‡é•¿
- **ä¼˜åŒ–ç­–ç•¥ï¼š**
```python
# KVç¼“å­˜ä¼˜åŒ–
class CachedAttention:
    def __init__(self):
        self.kv_cache = {}
    
    def forward(self, query, key, value, step):
        if step > 0:
            # å¤ç”¨ä¹‹å‰è®¡ç®—çš„Kã€V
            key = torch.cat([self.kv_cache['key'], key], dim=1)
            value = torch.cat([self.kv_cache['value'], value], dim=1)
        
        self.kv_cache = {'key': key, 'value': value}
        return self.attention(query, key, value)
```

### ğŸ“Š æ€§èƒ½ç›‘æ§ä¸åˆ†æ

**å…³é”®æŒ‡æ ‡ç›‘æ§ï¼š**

```python
class TransformerTrainer:
    def __init__(self):
        self.metrics = {
            'loss': [],
            'perplexity': [],
            'gpu_memory': [],
            'training_speed': []
        }
    
    def log_metrics(self, loss, batch_size, elapsed_time):
        # å›°æƒ‘åº¦ï¼šè¡¡é‡æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§
        perplexity = torch.exp(loss)
        
        # GPUå†…å­˜ä½¿ç”¨ç‡
        gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
        
        # è®­ç»ƒé€Ÿåº¦ï¼ˆtokens/ç§’ï¼‰
        speed = batch_size / elapsed_time
        
        self.metrics['loss'].append(loss.item())
        self.metrics['perplexity'].append(perplexity.item())
        self.metrics['gpu_memory'].append(gpu_memory)
        self.metrics['training_speed'].append(speed)
```

## ä»ç©å…·æ¨¡å‹åˆ°ç”Ÿäº§çº§ç³»ç»Ÿ

### ğŸ® ç¬¬ä¸€æ­¥ï¼šæ„å»ºç©å…·æ¨¡å‹

**ç›®æ ‡ï¼š** åœ¨å°æ•°æ®é›†ä¸ŠéªŒè¯æ¶æ„æ­£ç¡®æ€§

```python
# è¶…å°å‹Transformeré…ç½®
config = {
    'vocab_size': 1000,
    'd_model': 128,
    'num_heads': 4,
    'num_layers': 2,
    'max_seq_length': 64
}

# ä½¿ç”¨ç®€å•ä»»åŠ¡éªŒè¯ï¼ˆå¦‚åºåˆ—å¤åˆ¶ï¼‰
def test_copy_task():
    model = MiniTransformer(config)
    
    # è¾“å…¥ï¼š[1, 2, 3, 4, 5]
    # æœŸæœ›è¾“å‡ºï¼š[1, 2, 3, 4, 5]
    input_seq = torch.tensor([[1, 2, 3, 4, 5]])
    output = model(input_seq)
    
    return torch.argmax(output, dim=-1)
```

### ğŸš€ ç¬¬äºŒæ­¥ï¼šæ‰©å±•åˆ°å®é™…ä»»åŠ¡

**æ¸è¿›å¼æ‰©å±•ç­–ç•¥ï¼š**

1. **å¢åŠ æ¨¡å‹å®¹é‡**ï¼šæ›´å¤šå±‚æ•°ã€æ›´å¤§éšè—ç»´åº¦
2. **å¼•å…¥æ›´å¤æ‚çš„æ•°æ®**ï¼šçœŸå®æ–‡æœ¬ã€å¤šè¯­è¨€æ•°æ®
3. **æ·»åŠ é«˜çº§ç‰¹æ€§**ï¼šbeam searchã€æ¸©åº¦é‡‡æ ·

```python
# ç”Ÿäº§çº§é…ç½®ç¤ºä¾‹
production_config = {
    'vocab_size': 50000,
    'd_model': 512,
    'num_heads': 8,
    'num_layers': 6,
    'max_seq_length': 512,
    'dropout': 0.1,
    'label_smoothing': 0.1
}
```

## æ·±åº¦æ€è€ƒï¼šæ¶æ„è®¾è®¡çš„å“²å­¦

### ğŸ¤” ä¸ºä»€ä¹ˆTransformerå¦‚æ­¤æˆåŠŸï¼Ÿ

é€šè¿‡äº²æ‰‹æ­å»ºTransformerï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ·±åˆ»ç†è§£å…¶æˆåŠŸçš„æ ¹æœ¬åŸå› ï¼š

1. **å¹¶è¡Œæ€§**ï¼šæ‰“ç ´äº†RNNçš„é¡ºåºçº¦æŸ
2. **å¯æ‰©å±•æ€§**ï¼šæ¶æ„è®¾è®¡æ”¯æŒä»»æ„è§„æ¨¡æ‰©å±•
3. **é€šç”¨æ€§**ï¼šåŒä¸€æ¶æ„é€‚ç”¨äºå¤šç§NLPä»»åŠ¡
4. **å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡æä¾›äº†æ¨¡å‹å†³ç­–çš„é€æ˜åº¦

### ğŸ’¡ è®¾è®¡åŸåˆ™çš„å¯å‘

**æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶èŒè´£æ˜ç¡®ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
**æ•°å­¦ä¼˜é›…æ€§**ï¼šç®€æ´çš„æ•°å­¦å…¬å¼èƒŒåè•´å«æ·±åˆ»çš„æ´å¯Ÿ
**å·¥ç¨‹å®ç”¨æ€§**ï¼šç†è®ºåˆ›æ–°ä¸å·¥ç¨‹å®ç°çš„å®Œç¾ç»“åˆ

## å­¦ä¹ èµ„æºä¸è¿›é˜¶æ–¹å‘

### ğŸ“š æ¨èå­¦ä¹ è·¯å¾„

1. **ç†è®ºåŸºç¡€**ï¼š
   - ã€ŠAttention Is All You Needã€‹åŸè®ºæ–‡æ·±åº¦è§£è¯»
   - çº¿æ€§ä»£æ•°å’Œæ¦‚ç‡è®ºçš„æ•°å­¦åŸºç¡€

2. **ä»£ç å®è·µ**ï¼š
   - ä»é›¶å®ç°mini-Transformer
   - é˜…è¯»Hugging Face Transformersæºç 
   - å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®

3. **å·¥ç¨‹ä¼˜åŒ–**ï¼š
   - åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯
   - æ¨¡å‹å‹ç¼©ä¸é‡åŒ–
   - æ¨ç†åŠ é€ŸæŠ€æœ¯

### ğŸ”® æœªæ¥å‘å±•æ–¹å‘

**æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿ï¼š**
- **æ•ˆç‡ä¼˜åŒ–**ï¼šLinear Attentionã€Sparse Attention
- **æ¶æ„åˆ›æ–°**ï¼šMoEï¼ˆä¸“å®¶æ··åˆï¼‰ã€RetNet
- **åº”ç”¨æ‹“å±•**ï¼šå¤šæ¨¡æ€Transformerã€ç§‘å­¦è®¡ç®—

## æ€»ç»“ï¼šä»ç†è§£åˆ°æŒæ¡çš„è·ƒè¿

æ­å»ºTransformerä¸ä»…ä»…æ˜¯ç¼–ç¨‹ç»ƒä¹ ï¼Œæ›´æ˜¯ä¸€æ¬¡æ·±åº¦çš„è®¤çŸ¥å‡çº§ã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬ï¼š

- **ç†è§£äº†æ¶æ„è®¾è®¡çš„æƒè¡¡**ï¼šæ€§èƒ½vså¤æ‚åº¦ã€é€šç”¨æ€§vsç‰¹åŒ–
- **æŒæ¡äº†å·¥ç¨‹å®è·µçš„æŠ€å·§**ï¼šè°ƒè¯•ã€ä¼˜åŒ–ã€éƒ¨ç½²
- **åŸ¹å…»äº†ç³»ç»Ÿæ€ç»´**ï¼šä»ç»„ä»¶åˆ°æ•´ä½“ã€ä»ç†è®ºåˆ°åº”ç”¨

**æœ€é‡è¦çš„æ˜¯**ï¼Œæˆ‘ä»¬è·å¾—äº†æ”¹è¿›å’Œåˆ›æ–°çš„èƒ½åŠ›ã€‚å½“æˆ‘ä»¬çœŸæ­£ç†è§£äº†Transformerçš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼Œæˆ‘ä»¬å°±å…·å¤‡äº†è®¾è®¡ä¸‹ä¸€ä»£æ¶æ„çš„åŸºç¡€ã€‚

è¿™å°±æ˜¯ä»"çŸ¥å…¶ç„¶"åˆ°"çŸ¥å…¶æ‰€ä»¥ç„¶"ï¼Œå†åˆ°"èƒ½å¤Ÿåˆ›é€ "çš„å®Œæ•´å­¦ä¹ é—­ç¯ã€‚

---

## å‚è€ƒèµ„æº

### æ•™ç¨‹é“¾æ¥
- [Datawhale Happy-LLM æ•™ç¨‹](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter2/ç¬¬äºŒç« %20Transformeræ¶æ„.md)
- [é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¯¦è§£](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter3/ç¬¬ä¸‰ç« %20é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹.md)

### ä¼˜ç§€ç¬”è®°å‚è€ƒ
- [Datawhale Task04 å­¦ä¹ ç¬”è®°](https://www.notion.so/Datawhale-happy-llm-Task04-Encoder-Decoder-21a8bd3e44178040a746fbd52fb7d715)
- [Transformer å®ç°è¯¦è§£](https://lti4dfmqrs.feishu.cn/docx/LBp1d58YwoyRaxxj3W4c5fjcn2g)

### ä»£ç å®ç°
- [å®˜æ–¹ Transformer å®ç°](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter2/code/transformer.py)