# Task07：3.2 Encoder-Decoder PLM 

想象一下，如果你要设计一个既能深度理解文本，又能流畅生成文本的AI系统，你会怎么做？单纯的Encoder-only模型擅长理解但无法生成，Decoder-only模型能生成但理解能力有限。那么，**能否将两者的优势结合起来？**

这就是Encoder-Decoder预训练语言模型的核心思想：**统一理解与生成**。

让我们从认知科学的角度思考：当你在翻译一段文字时，你的大脑是如何工作的？

1. **理解阶段**：深度分析源文本的语义、语法、上下文
2. **转换阶段**：在语义空间中进行跨语言映射
3. **生成阶段**：按照目标语言的规则逐步生成文本

这正是Encoder-Decoder架构的设计哲学：**专门化分工，协同合作**。

### T5：Text-to-Text Transfer Transformer的革命

把所有NLP任务都可以统一为文本到文本的转换。

```
翻译：translate English to German: Hello → Hallo
摘要：summarize: [长文本] → [摘要]
问答：question: What is AI? context: [文档] → AI is...
分类：classify sentiment: I love this movie → positive
```

这种统一范式带来了什么好处？又有什么局限性？

Encoder：双向注意力的力量

```python
# Encoder的核心：双向自注意力
class EncoderLayer(nn.Module):
    def forward(self, x, mask=None):
        # 双向自注意力：每个位置都能看到所有位置
        attn_output = self.self_attention(x, x, x, mask)
        # 前馈网络
        output = self.feed_forward(attn_output)
        return output
```

为什么Encoder可以使用双向注意力？这与Decoder有什么本质区别？

想象Encoder就像一个深度阅读理解专家：
- **底层**：词汇、语法特征
- **中层**：句法结构、语义角色
- **高层**：语篇理解、逻辑关系

每一层都在前一层的基础上构建更抽象的表示。

Decoder：创造性生成的机制


```python
# Decoder的约束：因果掩码
def create_causal_mask(seq_len):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask.bool()  # 上三角为True，表示不能看到

# 为什么需要这个约束？
# 生成时，第i个位置不能看到第i+1个位置的信息
```

这种因果性约束是技术限制还是本质需求？

交叉注意力

```python
class DecoderLayer(nn.Module):
    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # 1. 自注意力（因果掩码）
        x = self.self_attention(x, x, x, tgt_mask)
        
        # 2. 交叉注意力（关键！）
        x = self.cross_attention(
            query=x,           # 来自decoder
            key=encoder_output, # 来自encoder
            value=encoder_output
        )
        
        # 3. 前馈网络
        x = self.feed_forward(x)
        return x
```

**核心洞察**：交叉注意力是Encoder-Decoder架构的灵魂，它实现了：
- 理解信息向生成过程的传递
- 动态的上下文感知生成
- 输入与输出的语义对齐

Span Corruption：更自然的掩码策略

```
原文：The cat sat on the mat and looked around
掩码：The cat <X> and <Y>
目标：<X> sat on the mat <Y> looked around
```

相比BERT的单词掩码，Span Corruption有什么优势？
1. **更自然**：模拟真实的文本缺失场景
2. **更具挑战性**：需要理解更长的上下文
3. **更适合生成**：训练模型生成连续文本

#### 多任务预训练的智慧

```python
# T5的多任务预训练
tasks = {
    'span_corruption': 0.5,    # 主要任务
    'translation': 0.1,        # 翻译任务
    'summarization': 0.1,      # 摘要任务
    'question_answering': 0.1, # 问答任务
    'classification': 0.2      # 分类任务
}
```

**思考**：为什么多任务预训练比单任务更有效？这背后的原理是什么？

BART：多样化的噪声策略

```python
# BART的噪声函数
def add_noise(text):
    strategies = [
        token_masking,      # 词汇掩码
        token_deletion,     # 词汇删除
        text_infilling,     # 文本填充
        sentence_permutation, # 句子重排
        document_rotation   # 文档旋转
    ]
    return random.choice(strategies)(text)
```

不同的噪声策略训练模型的不同能力：
- **掩码**：局部理解和生成
- **删除**：信息压缩和恢复
- **重排**：结构理解和重组
- **旋转**：全局一致性维护

预训练与微调的一致性

**重要原则**：预训练任务应该与下游任务尽可能一致。

BART的去噪预训练天然适合：
- 文本摘要（信息压缩）
- 文本生成（创造性输出）
- 机器翻译（跨语言转换）


Teacher Forcing vs 自回归生成


训练时：Teacher Forcing

```python
# 训练时的并行计算
def training_step(encoder_input, decoder_input, target):
    encoder_output = encoder(encoder_input)
    
    # 所有位置同时计算（并行）
    decoder_output = decoder(
        decoder_input,      # 包含所有目标token
        encoder_output,
        causal_mask=True
    )
    
    loss = cross_entropy(decoder_output, target)
    return loss
```

推理时：自回归生成

```python
# 推理时的逐步生成
def inference(encoder_input, max_length=100):
    encoder_output = encoder(encoder_input)
    
    generated = [BOS_TOKEN]
    for i in range(max_length):
        decoder_output = decoder(
            generated,          # 只包含已生成的token
            encoder_output
        )
        
        next_token = sample(decoder_output[-1])  # 只取最后一个位置
        generated.append(next_token)
        
        if next_token == EOS_TOKEN:
            break
    
    return generated
```

**关键问题**：训练和推理的不一致性（Exposure Bias）如何解决？

生成策略的选择：贪心解码 vs 束搜索 vs 采样

```python
# 不同的解码策略
def greedy_decode(logits):
    return torch.argmax(logits, dim=-1)

def beam_search(logits, beam_size=5):
    # 保持top-k个候选序列
    return beam_search_algorithm(logits, beam_size)

def nucleus_sampling(logits, p=0.9):
    # 从累积概率为p的核心词汇中采样
    return nucleus_sample(logits, p)
```

**思考**：
- 什么时候用贪心？（确定性任务）
- 什么时候用束搜索？（质量优先）
- 什么时候用采样？（多样性优先）

实际应用：长序列处理的困境、注意力复杂度的挑战

```
标准注意力：O(n²) 复杂度
输入长度1024：1M 计算量
输入长度4096：16M 计算量（16倍增长！）
```

解决方案探索：

1. **局部注意力**：
```python
# 滑动窗口注意力
def local_attention(q, k, v, window_size=512):
    # 每个位置只关注局部窗口
    return windowed_attention(q, k, v, window_size)
```

2. **稀疏注意力**：
```python
# Longformer的注意力模式
attention_pattern = {
    'local': True,      # 局部注意力
    'global': [0, -1],  # 全局注意力位置
    'random': 0.1       # 随机注意力比例
}
```

3. **层次化处理**：
```python
# 分层编码策略
def hierarchical_encoding(long_text):
    # 1. 句子级编码
    sentence_embeddings = [encode(sent) for sent in sentences]
    
    # 2. 文档级编码
    document_embedding = encode(sentence_embeddings)
    
    return document_embedding
```

mT5：多语言统一模型

**设计理念**：一个模型处理100+种语言

```python
# 多语言任务统一格式
tasks = {
    'en_to_zh': 'translate English to Chinese: Hello',
    'zh_to_en': 'translate Chinese to English: 你好',
    'en_summary': 'summarize: [English text]',
    'zh_summary': 'summarize: [Chinese text]'
}
```


跨模态扩展的可能性：

```python
# 统一的多模态框架
class MultimodalEncoderDecoder(nn.Module):
    def __init__(self):
        self.text_encoder = TextEncoder()
        self.image_encoder = ImageEncoder()
        self.audio_encoder = AudioEncoder()
        self.unified_decoder = UnifiedDecoder()
    
    def forward(self, text=None, image=None, audio=None):
        # 多模态编码
        encodings = []
        if text: encodings.append(self.text_encoder(text))
        if image: encodings.append(self.image_encoder(image))
        if audio: encodings.append(self.audio_encoder(audio))
        
        # 统一解码
        return self.unified_decoder(encodings)
```

内存优化策略：

#### 梯度检查点（Gradient Checkpointing）

```python
# 用计算换内存
class CheckpointedEncoderLayer(nn.Module):
    def forward(self, x):
        # 不保存中间激活，反向传播时重新计算
        return checkpoint(self._forward, x)
    
    def _forward(self, x):
        return self.layer(x)
```

**权衡**：
- 内存使用减少50-80%
- 训练时间增加20-30%

#### 混合精度训练

```python
# FP16 + FP32 混合精度
with autocast():
    encoder_output = encoder(input_ids.half())
    decoder_output = decoder(decoder_input.half(), encoder_output)
    loss = criterion(decoder_output.float(), targets.float())

# 梯度缩放防止下溢
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 分布式训练策略

#### 数据并行 vs 模型并行

```python
# 数据并行：每个GPU处理不同的数据
model = nn.DataParallel(model)

# 模型并行：模型分布在多个GPU上
class PipelineParallelModel(nn.Module):
    def __init__(self):
        self.encoder = encoder.to('cuda:0')
        self.decoder = decoder.to('cuda:1')
    
    def forward(self, x):
        x = self.encoder(x)  # GPU 0
        x = x.to('cuda:1')
        x = self.decoder(x)  # GPU 1
        return x
```

#### ZeRO优化器状态分片

```python
# DeepSpeed ZeRO：优化器状态分片
from deepspeed import zero

# 将优化器状态分布到多个GPU
optimizer = zero.Init(optimizer, 
                     partition_optimizer=True,
                     cpu_offload=True)
```

## 评估与分析

### 自动评估指标的局限性

#### BLEU分数的问题

```python
# BLEU只考虑n-gram重叠
reference = "The cat sat on the mat"
candidate1 = "The cat sat on the mat"      # BLEU = 1.0
candidate2 = "A feline rested on the rug"  # BLEU ≈ 0.0

# 但candidate2可能语义更丰富！
```

**思考**：如何设计更好的评估指标？

#### 语义相似度评估

```python
# 基于嵌入的语义评估
def semantic_similarity(ref, cand):
    ref_emb = sentence_encoder(ref)
    cand_emb = sentence_encoder(cand)
    return cosine_similarity(ref_emb, cand_emb)

# BERTScore：基于BERT的评估
from bert_score import score
P, R, F1 = score(candidates, references, lang='en')
```


### UL2：统一语言学习框架

#### 多种预训练目标的融合

```python
# UL2的统一框架
pretraining_objectives = {
    'prefix_lm': 0.25,      # 前缀语言模型
    'span_corruption': 0.25, # 跨度破坏
    'packing': 0.25,        # 序列打包
    'mixture': 0.25         # 混合目标
}
```

**核心思想**：不同的预训练目标训练模型的不同能力，统一框架能够综合这些能力。

### PaLM-2和Flan-T5的启示

#### 指令调优的重要性

```python
# 指令调优的数据格式
instruction_data = {
    'instruction': 'Summarize the following article in 3 sentences.',
    'input': '[Long article text]',
    'output': '[3-sentence summary]'
}
```

**关键**：
- 指令调优比规模扩大更重要
- 多样化的任务格式提升泛化能力
- 人类反馈对齐至关重要

### 效率与性能的新平衡

#### 专家混合（Mixture of Experts）

```python
class MoELayer(nn.Module):
    def __init__(self, num_experts=8, expert_dim=2048):
        self.experts = nn.ModuleList([
            FeedForward(expert_dim) for _ in range(num_experts)
        ])
        self.gate = nn.Linear(hidden_dim, num_experts)
    
    def forward(self, x):
        # 路由到不同的专家
        gate_scores = self.gate(x)
        expert_weights = softmax(gate_scores)
        
        # 只激活top-k个专家
        top_k_experts = torch.topk(expert_weights, k=2)
        
        output = 0
        for expert_id, weight in top_k_experts:
            output += weight * self.experts[expert_id](x)
        
        return output
```

**优势**：
- 参数量大但计算量可控
- 不同专家学习不同技能
- 更好的参数效率

## 思考

1. **架构哲学**：Encoder-Decoder架构的分离是必然还是偶然？未来是否会出现更统一的架构？

2. **预训练范式**：Text-to-Text统一范式是NLP的终极形态吗？还有什么其他可能的统一方式？

3. **生成质量**：如何在生成的流畅性、准确性和创新性之间找到平衡？这是技术问题还是哲学问题？

4. **多模态融合**：如何真正实现多模态的深度融合，而不仅仅是简单的拼接？

5. **效率与能力**：在追求更强能力的同时，如何保持计算效率？这两者是否存在根本矛盾？

6. **评估困境**：如何设计真正反映模型能力的评估指标？自动评估能否替代人工评估？

## 实践项目建议

### 初级项目
1. **微调T5进行文本摘要**：
   - 使用CNN/DailyMail数据集
   - 对比不同解码策略的效果
   - 分析注意力模式

2. **构建简单的翻译系统**：
   - 使用预训练的mT5模型
   - 实现多种评估指标
   - 分析错误案例

### 中级项目
1. **实现BART的预训练**：
   - 设计多种噪声策略
   - 对比不同策略的效果
   - 可视化学习过程

2. **多任务学习框架**：
   - 统一多个NLP任务
   - 设计任务权重调度策略
   - 分析任务间的相互影响

### 高级项目
1. **长文档处理系统**：
   - 实现层次化注意力
   - 对比不同长序列处理方法
   - 优化内存使用

2. **多模态Encoder-Decoder**：
   - 融合文本和图像信息
   - 实现跨模态生成
   - 设计新的评估方法

## 学习资源

### 核心论文
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation
- mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
- UL2: Unifying Language Learning Paradigms

### 实践资源
- [Happy-LLM教程](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter3/第三章%20预训练语言模型.md)
- Hugging Face Transformers库
- Google T5官方代码
- Facebook BART实现

### 进阶阅读
- "The Illustrated T5" - 可视化解释
- "BART Explained" - 技术博客
- 相关论文的复现代码
- 最新的多模态研究

---

**记住**：Encoder-Decoder架构不仅仅是技术的组合，更是对理解与生成这两个核心AI能力的深刻思考。掌握这种架构，就是掌握了现代NLP的核心范式。

**下一步**：尝试微调一个T5模型，或者实现一个简单的BART预训练，感受理解与生成的统一之美。