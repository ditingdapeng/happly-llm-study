# Task05ï¼šæ­å»ºTransformerå®è·µ - é¢è¯•é—®é¢˜é›†

## åŸºç¡€å®ç°ç±»é—®é¢˜

### Q1: Transformeræœ€å°å¯è¿è¡Œå•å…ƒ
**é—®é¢˜ï¼š** å¦‚æœè¦ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªæœ€ç®€å•ä½†å®Œæ•´çš„Transformerï¼Œä½ è®¤ä¸ºæœ€å°‘éœ€è¦å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿè¯·è§£é‡Šæ¯ä¸ªç»„ä»¶çš„å¿…è¦æ€§ã€‚

**è€ƒæŸ¥ç‚¹ï¼š**
- å¯¹Transformeræ¶æ„çš„æ•´ä½“ç†è§£
- ç»„ä»¶é—´ä¾èµ–å…³ç³»çš„è®¤çŸ¥
- æœ€å°å¯è¡Œäº§å“çš„è®¾è®¡æ€ç»´

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```plaintext
æ ¸å¿ƒç»„ä»¶ï¼ˆç¼ºä¸€ä¸å¯ï¼‰ï¼š
1. åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰ï¼š
   - å°†ç¦»æ•£tokenè½¬æ¢ä¸ºè¿ç»­å‘é‡
   - æä¾›æ¨¡å‹å¯å¤„ç†çš„æ•°å€¼è¡¨ç¤º

2. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼š
   - æ³¨å…¥åºåˆ—ä½ç½®ä¿¡æ¯
   - å¼¥è¡¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½ç½®æ— å…³çš„ç¼ºé™·

3. å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ï¼š
   - æ ¸å¿ƒè®¡ç®—æœºåˆ¶
   - æ•è·åºåˆ—å†…éƒ¨ä¾èµ–å…³ç³»

4. å‰é¦ˆç½‘ç»œï¼ˆFeed-Forwardï¼‰ï¼š
   - æä¾›éçº¿æ€§å˜æ¢èƒ½åŠ›
   - å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

5. è¾“å‡ºæŠ•å½±å±‚ï¼ˆOutput Projectionï¼‰ï¼š
   - å°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨
   - ç”Ÿæˆæœ€ç»ˆé¢„æµ‹æ¦‚ç‡

å¯é€‰ä½†é‡è¦çš„ç»„ä»¶ï¼š
- å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰
- æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰
- Dropoutæ­£åˆ™åŒ–
```

---

### Q2: å¤šå¤´æ³¨æ„åŠ›çš„å…·ä½“å®ç°
**é—®é¢˜ï¼š** è¯·è¯¦ç»†è§£é‡Šå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°è¿‡ç¨‹ï¼Œä¸ºä»€ä¹ˆè¦è¿›è¡Œç»´åº¦é‡å¡‘ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- å¼ é‡æ“ä½œçš„ç†è§£
- å¤šå¤´æ³¨æ„åŠ›çš„æ•°å­¦åŸç†
- å¹¶è¡Œè®¡ç®—çš„è®¾è®¡æ€æƒ³

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# å®ç°æ­¥éª¤è¯¦è§£
class MultiHeadAttention(nn.Module):
    def forward(self, query, key, value):
        batch_size, seq_len, d_model = query.size()
        
        # 1. çº¿æ€§å˜æ¢ï¼š[B, L, D] -> [B, L, D]
        Q = self.W_q(query)
        K = self.W_k(key) 
        V = self.W_v(value)
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ï¼š[B, L, D] -> [B, H, L, D/H]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 3. å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—
        attention_output = self.scaled_dot_product_attention(Q, K, V)
        
        # 4. é‡æ–°ç»„åˆï¼š[B, H, L, D/H] -> [B, L, D]
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.W_o(attention_output)

# ä¸ºä»€ä¹ˆè¦é‡å¡‘ç»´åº¦ï¼Ÿ
1. å¹¶è¡Œè®¡ç®—ï¼šå¤šä¸ªå¤´å¯ä»¥åŒæ—¶è®¡ç®—ï¼Œæé«˜æ•ˆç‡
2. ç‹¬ç«‹æ€§ï¼šæ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
3. å†…å­˜å¸ƒå±€ï¼šä¼˜åŒ–GPUè®¡ç®—çš„å†…å­˜è®¿é—®æ¨¡å¼
```

---

### Q3: ä½ç½®ç¼–ç çš„è®¾è®¡é€‰æ‹©
**é—®é¢˜ï¼š** Transformerä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç ï¼Œè¯·è§£é‡Šå…¶æ•°å­¦åŸç†å’Œä¼˜åŠ¿ã€‚å¦‚æœè®©ä½ è®¾è®¡ä½ç½®ç¼–ç ï¼Œä½ ä¼šè€ƒè™‘å“ªäº›å› ç´ ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- ä½ç½®ç¼–ç çš„æ•°å­¦ç†è§£
- è®¾è®¡æƒè¡¡çš„æ€è€ƒ
- åˆ›æ–°æ€ç»´èƒ½åŠ›

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# æ­£å¼¦ä½ç½®ç¼–ç çš„æ•°å­¦åŸç†
def positional_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1).float()
    
    # ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                        -(math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦
    pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦
    
    return pe

# æ­£å¼¦ç¼–ç çš„ä¼˜åŠ¿ï¼š
1. ç¡®å®šæ€§ï¼šä¸éœ€è¦å­¦ä¹ å‚æ•°
2. å¤–æ¨æ€§ï¼šå¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦
3. ç›¸å¯¹ä½ç½®ï¼šsin(pos+k)å¯ä»¥è¡¨ç¤ºä¸ºsin(pos)å’Œcos(pos)çš„çº¿æ€§ç»„åˆ
4. å‘¨æœŸæ€§ï¼šæä¾›äº†ä¸°å¯Œçš„ä½ç½®æ¨¡å¼

# è®¾è®¡è€ƒè™‘å› ç´ ï¼š
1. é•¿åº¦æ³›åŒ–èƒ½åŠ›
2. ç›¸å¯¹ä½ç½®vsç»å¯¹ä½ç½®
3. è®¡ç®—å¤æ‚åº¦
4. ä¸ä»»åŠ¡çš„åŒ¹é…åº¦
5. å¯å­¦ä¹ æ€§vså›ºå®šæ€§
```

---

## å·¥ç¨‹å®è·µç±»é—®é¢˜

### Q4: åºåˆ—é•¿åº¦å¤„ç†ç­–ç•¥
**é—®é¢˜ï¼š** åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¾“å…¥åºåˆ—é•¿åº¦å„ä¸ç›¸åŒï¼Œä½ å¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Ÿæœ‰å“ªäº›ç­–ç•¥å’Œæƒè¡¡ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- å®é™…å·¥ç¨‹é—®é¢˜çš„ç†è§£
- æ‰¹å¤„ç†ä¼˜åŒ–çš„æ€è€ƒ
- æ€§èƒ½ä¸ç²¾åº¦çš„æƒè¡¡

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ä¸»è¦ç­–ç•¥å¯¹æ¯”

1. Padding + Maskingï¼ˆæœ€å¸¸ç”¨ï¼‰ï¼š
ä¼˜åŠ¿ï¼š
- å®ç°ç®€å•ï¼Œæ”¯æŒæ‰¹å¤„ç†
- ä¸æŸå¤±ä¿¡æ¯
ç¼ºç‚¹ï¼š
- è®¡ç®—æµªè´¹ï¼ˆå¯¹paddingä½ç½®çš„è®¡ç®—ï¼‰
- å†…å­˜å ç”¨å¤§

å®ç°ï¼š
def create_padding_mask(sequences, pad_token=0):
    return (sequences != pad_token).unsqueeze(1).unsqueeze(2)

2. åŠ¨æ€æ‰¹å¤„ç†ï¼ˆDynamic Batchingï¼‰ï¼š
ä¼˜åŠ¿ï¼š
- å‡å°‘paddingæµªè´¹
- æé«˜è®¡ç®—æ•ˆç‡
ç¼ºç‚¹ï¼š
- å®ç°å¤æ‚
- æ‰¹å¤§å°ä¸å›ºå®š

3. åˆ†æ¡¶ç­–ç•¥ï¼ˆBucketingï¼‰ï¼š
ä¼˜åŠ¿ï¼š
- å¹³è¡¡æ•ˆç‡å’Œç®€å•æ€§
- å‡å°‘paddingæ•°é‡
ç¼ºç‚¹ï¼š
- éœ€è¦é¢„å¤„ç†
- å¯èƒ½å½±å“éšæœºæ€§

4. æˆªæ–­ç­–ç•¥ï¼š
ä¼˜åŠ¿ï¼š
- æ§åˆ¶è®¡ç®—å¤æ‚åº¦
- å†…å­˜ä½¿ç”¨å¯é¢„æµ‹
ç¼ºç‚¹ï¼š
- ä¿¡æ¯ä¸¢å¤±
- å¯èƒ½å½±å“æ€§èƒ½

# é€‰æ‹©ç­–ç•¥çš„è€ƒè™‘å› ç´ ï¼š
- åºåˆ—é•¿åº¦åˆ†å¸ƒ
- è®¡ç®—èµ„æºé™åˆ¶
- ä»»åŠ¡å¯¹é•¿åºåˆ—çš„æ•æ„Ÿåº¦
- å®ç°å¤æ‚åº¦è¦æ±‚
```

---

### Q5: å†…å­˜ä¼˜åŒ–æŠ€æœ¯
**é—®é¢˜ï¼š** è®­ç»ƒå¤§å‹Transformeræ—¶ç»å¸¸é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œä½ çŸ¥é“å“ªäº›å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Ÿè¯·è§£é‡Šå…¶åŸç†ã€‚

**è€ƒæŸ¥ç‚¹ï¼š**
- æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¼˜åŒ–çš„ç†è§£
- å†…å­˜ç®¡ç†çš„æŠ€æœ¯ç»†èŠ‚
- å®é™…éƒ¨ç½²ç»éªŒ

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ä¸»è¦ä¼˜åŒ–æŠ€æœ¯

1. æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰ï¼š
åŸç†ï¼šæ¨¡æ‹Ÿå¤§batchè®­ç»ƒï¼Œåˆ†å¤šä¸ªå°batchç´¯ç§¯æ¢¯åº¦

accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

2. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰ï¼š
åŸç†ï¼šä½¿ç”¨FP16è¿›è¡Œå‰å‘ä¼ æ’­ï¼ŒFP32è¿›è¡Œæ¢¯åº¦æ›´æ–°

from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

3. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ï¼š
åŸç†ï¼šé‡è®¡ç®—éƒ¨åˆ†ä¸­é—´ç»“æœï¼Œç”¨æ—¶é—´æ¢ç©ºé—´

from torch.utils.checkpoint import checkpoint

class TransformerLayer(nn.Module):
    def forward(self, x):
        return checkpoint(self._forward, x)

4. æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰ï¼š
åŸç†ï¼šå°†æ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ªGPUä¸Š

# ä¸åŒå±‚æ”¾åœ¨ä¸åŒGPU
self.layer1 = layer1.to('cuda:0')
self.layer2 = layer2.to('cuda:1')

5. ZeROä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼š
åŸç†ï¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†å¸ƒåˆ°å¤šä¸ªè®¾å¤‡

# ä½¿ç”¨DeepSpeed ZeRO
from deepspeed import zero
optimizer = zero.Init(optimizer)

# é€‰æ‹©ç­–ç•¥çš„è€ƒè™‘ï¼š
- æ¨¡å‹å¤§å°vså¯ç”¨å†…å­˜
- è®­ç»ƒé€Ÿåº¦è¦æ±‚
- å®ç°å¤æ‚åº¦
- æ•°å€¼ç¨³å®šæ€§
```

---

### Q6: è®­ç»ƒç¨³å®šæ€§æŠ€å·§
**é—®é¢˜ï¼š** Transformerè®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°å“ªäº›ç¨³å®šæ€§é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- è®­ç»ƒè¿‡ç¨‹çš„æ·±åº¦ç†è§£
- é—®é¢˜è¯Šæ–­èƒ½åŠ›
- è§£å†³æ–¹æ¡ˆçš„æŒæ¡

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼š
ç—‡çŠ¶ï¼šæŸå¤±çªç„¶å˜ä¸ºNaNæˆ–ä¸æ”¶æ•›
è§£å†³ï¼š
# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æƒé‡åˆå§‹åŒ–
def init_weights(module):
    if isinstance(module, nn.Linear):
        torch.nn.init.xavier_uniform_(module.weight)
        module.bias.data.fill_(0.01)

2. å­¦ä¹ ç‡è°ƒåº¦é—®é¢˜ï¼š
ç—‡çŠ¶ï¼šè®­ç»ƒåˆæœŸä¸ç¨³å®šæˆ–åæœŸä¸æ”¶æ•›
è§£å†³ï¼š
class WarmupLRScheduler:
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
    
    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

3. æ•°å€¼ä¸ç¨³å®šï¼š
ç—‡çŠ¶ï¼šæ³¨æ„åŠ›æƒé‡å‡ºç°æå€¼
è§£å†³ï¼š
# æ³¨æ„åŠ›åˆ†æ•°ç¼©æ”¾
scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

# æ ‡ç­¾å¹³æ»‘
class LabelSmoothingLoss(nn.Module):
    def __init__(self, vocab_size, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.vocab_size = vocab_size
    
    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (self.vocab_size - 1))
        true_dist.scatter_(1, target.unsqueeze(1), confidence)
        return F.kl_div(F.log_softmax(pred, dim=1), true_dist)

4. è¿‡æ‹Ÿåˆï¼š
ç—‡çŠ¶ï¼šè®­ç»ƒé›†æ€§èƒ½å¥½ï¼ŒéªŒè¯é›†æ€§èƒ½å·®
è§£å†³ï¼š
# Dropout
self.dropout = nn.Dropout(0.1)

# æƒé‡è¡°å‡
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)

# æ—©åœ
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience
```

---

## æ€§èƒ½ä¼˜åŒ–ç±»é—®é¢˜

### Q7: æ¨ç†åŠ é€ŸæŠ€æœ¯
**é—®é¢˜ï¼š** åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²Transformeræ¨¡å‹æ—¶ï¼Œå¦‚ä½•ä¼˜åŒ–æ¨ç†é€Ÿåº¦ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- ç”Ÿäº§éƒ¨ç½²ç»éªŒ
- æ¨ç†ä¼˜åŒ–æŠ€æœ¯
- æ€§èƒ½ä¸ç²¾åº¦æƒè¡¡

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ä¸»è¦ä¼˜åŒ–æŠ€æœ¯

1. KVç¼“å­˜ï¼ˆKV Cacheï¼‰ï¼š
åŸç†ï¼šç¼“å­˜ä¹‹å‰è®¡ç®—çš„Keyå’ŒValueï¼Œé¿å…é‡å¤è®¡ç®—

class CachedMultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.kv_cache = None
    
    def forward(self, query, key, value, use_cache=False):
        if use_cache and self.kv_cache is not None:
            # ä½¿ç”¨ç¼“å­˜çš„K, V
            cached_k, cached_v = self.kv_cache
            key = torch.cat([cached_k, key], dim=1)
            value = torch.cat([cached_v, value], dim=1)
        
        # è®¡ç®—æ³¨æ„åŠ›
        output = self.attention(query, key, value)
        
        if use_cache:
            self.kv_cache = (key, value)
        
        return output

2. æ‰¹å¤„ç†ä¼˜åŒ–ï¼š
åŸç†ï¼šåŒæ—¶å¤„ç†å¤šä¸ªåºåˆ—ï¼Œæé«˜GPUåˆ©ç”¨ç‡

# åŠ¨æ€æ‰¹å¤„ç†
class DynamicBatcher:
    def __init__(self, max_batch_size=32, max_wait_time=0.1):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
    
    def add_request(self, request):
        self.pending_requests.append(request)
        if len(self.pending_requests) >= self.max_batch_size:
            return self.process_batch()
        return None
    
    def process_batch(self):
        batch = self.pending_requests[:self.max_batch_size]
        self.pending_requests = self.pending_requests[self.max_batch_size:]
        return self.model.batch_forward(batch)

3. æ¨¡å‹é‡åŒ–ï¼š
åŸç†ï¼šä½¿ç”¨ä½ç²¾åº¦æ•°å€¼è¡¨ç¤ºï¼Œå‡å°‘è®¡ç®—å’Œå†…å­˜éœ€æ±‚

# åŠ¨æ€é‡åŒ–
import torch.quantization as quant

model_quantized = quant.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

# é™æ€é‡åŒ–
model.qconfig = quant.get_default_qconfig('fbgemm')
model_prepared = quant.prepare(model)
# æ ¡å‡†æ•°æ®...
model_quantized = quant.convert(model_prepared)

4. æ¨¡å‹è’¸é¦ï¼š
åŸç†ï¼šç”¨å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹çš„çŸ¥è¯†

class DistillationLoss(nn.Module):
    def __init__(self, temperature=3.0, alpha=0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
    
    def forward(self, student_logits, teacher_logits, labels):
        # è½¯æ ‡ç­¾æŸå¤±
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=1),
            F.softmax(teacher_logits / self.temperature, dim=1),
            reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # ç¡¬æ ‡ç­¾æŸå¤±
        hard_loss = F.cross_entropy(student_logits, labels)
        
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss

5. å›¾ä¼˜åŒ–ï¼š
åŸç†ï¼šä¼˜åŒ–è®¡ç®—å›¾ï¼Œå‡å°‘ä¸å¿…è¦çš„æ“ä½œ

# TorchScriptä¼˜åŒ–
model_scripted = torch.jit.script(model)
model_optimized = torch.jit.optimize_for_inference(model_scripted)

# ONNXå¯¼å‡º
torch.onnx.export(model, dummy_input, "model.onnx", 
                 opset_version=11, do_constant_folding=True)
```

---

### Q8: åˆ†å¸ƒå¼è®­ç»ƒå®ç°
**é—®é¢˜ï¼š** å¦‚ä½•å®ç°Transformerçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿæœ‰å“ªäº›å¹¶è¡Œç­–ç•¥ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- åˆ†å¸ƒå¼ç³»ç»Ÿç†è§£
- å¹¶è¡Œè®¡ç®—ç­–ç•¥
- å¤§è§„æ¨¡è®­ç»ƒç»éªŒ

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ä¸»è¦å¹¶è¡Œç­–ç•¥

1. æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰ï¼š
åŸç†ï¼šæ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡

# PyTorch DDP
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def train(rank, world_size):
    setup(rank, world_size)
    
    model = TransformerModel().to(rank)
    model = DDP(model, device_ids=[rank])
    
    # åˆ†å¸ƒå¼æ•°æ®åŠ è½½
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)
    
    for batch in dataloader:
        outputs = model(batch)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

2. æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰ï¼š
åŸç†ï¼šå°†æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†æ”¾åœ¨ä¸åŒGPUä¸Š

class PipelineTransformer(nn.Module):
    def __init__(self, num_layers, devices):
        super().__init__()
        self.devices = devices
        layers_per_device = num_layers // len(devices)
        
        for i, device in enumerate(devices):
            start_layer = i * layers_per_device
            end_layer = (i + 1) * layers_per_device
            
            setattr(self, f'layers_{i}', 
                   nn.ModuleList([
                       TransformerLayer() for _ in range(start_layer, end_layer)
                   ]).to(device))
    
    def forward(self, x):
        for i, device in enumerate(self.devices):
            x = x.to(device)
            layers = getattr(self, f'layers_{i}')
            for layer in layers:
                x = layer(x)
        return x

3. æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰ï¼š
åŸç†ï¼šå°†æ‰¹æ¬¡åˆ†æˆå¾®æ‰¹æ¬¡ï¼Œåœ¨ä¸åŒé˜¶æ®µå¹¶è¡Œå¤„ç†

from torch.distributed.pipeline.sync import Pipe

# åˆ›å»ºæµæ°´çº¿
model = nn.Sequential(
    TransformerLayer(),
    TransformerLayer(),
    TransformerLayer(),
    TransformerLayer()
)

# åˆ†é…åˆ°ä¸åŒè®¾å¤‡
model = Pipe(model, balance=[1, 1, 1, 1], devices=[0, 1, 2, 3])

4. å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰ï¼š
åŸç†ï¼šå°†å•ä¸ªå¼ é‡æ“ä½œåˆ†å¸ƒåˆ°å¤šä¸ªè®¾å¤‡

class ParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, world_size):
        super().__init__()
        self.world_size = world_size
        self.out_features_per_device = out_features // world_size
        
        self.weight = nn.Parameter(
            torch.randn(in_features, self.out_features_per_device)
        )
    
    def forward(self, x):
        # æœ¬åœ°è®¡ç®—
        local_output = F.linear(x, self.weight)
        
        # å…¨æ”¶é›†ç»“æœ
        output_list = [torch.zeros_like(local_output) for _ in range(self.world_size)]
        dist.all_gather(output_list, local_output)
        
        return torch.cat(output_list, dim=-1)

# é€‰æ‹©ç­–ç•¥çš„è€ƒè™‘å› ç´ ï¼š
1. æ¨¡å‹å¤§å° vs GPUå†…å­˜
2. é€šä¿¡å¼€é”€ vs è®¡ç®—æ•ˆç‡
3. å®ç°å¤æ‚åº¦
4. å®¹é”™èƒ½åŠ›
5. æ‰©å±•æ€§è¦æ±‚
```

---

## è°ƒè¯•ä¸è¯Šæ–­ç±»é—®é¢˜

### Q9: å¸¸è§è®­ç»ƒé—®é¢˜è¯Šæ–­
**é—®é¢˜ï¼š** åœ¨è®­ç»ƒTransformeræ—¶ï¼Œå¦‚ä½•å¿«é€Ÿè¯Šæ–­å’Œè§£å†³å¸¸è§é—®é¢˜ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- é—®é¢˜è¯Šæ–­èƒ½åŠ›
- è°ƒè¯•ç»éªŒ
- ç³»ç»Ÿæ€§æ€ç»´

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ç³»ç»Ÿæ€§è¯Šæ–­æµç¨‹

class TransformerDebugger:
    def __init__(self, model, dataloader):
        self.model = model
        self.dataloader = dataloader
        self.metrics = {}
    
    def diagnose_training_issues(self):
        """å…¨é¢è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        
        # 1. æ£€æŸ¥æ•°æ®
        self.check_data_quality()
        
        # 2. æ£€æŸ¥æ¨¡å‹
        self.check_model_health()
        
        # 3. æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹
        self.check_training_dynamics()
        
        # 4. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        return self.generate_report()
    
    def check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        batch = next(iter(self.dataloader))
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        input_ids = batch['input_ids']
        print(f"Input range: {input_ids.min()} - {input_ids.max()}")
        print(f"Vocab size: {self.model.config.vocab_size}")
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦åˆ†å¸ƒ
        lengths = (input_ids != 0).sum(dim=1)
        print(f"Sequence lengths: mean={lengths.float().mean():.1f}, "
              f"std={lengths.float().std():.1f}")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        if 'labels' in batch:
            labels = batch['labels']
            unique_labels = torch.unique(labels[labels != -100])
            print(f"Unique labels: {len(unique_labels)}")
    
    def check_model_health(self):
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
        
        # æ£€æŸ¥å‚æ•°åˆå§‹åŒ–
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                mean_val = param.data.mean().item()
                std_val = param.data.std().item()
                print(f"{name}: mean={mean_val:.6f}, std={std_val:.6f}")
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                if abs(mean_val) > 1.0 or std_val > 1.0:
                    print(f"WARNING: {name} may have initialization issues")
    
    def check_training_dynamics(self):
        """æ£€æŸ¥è®­ç»ƒåŠ¨æ€"""
        
        # å‰å‘ä¼ æ’­æ£€æŸ¥
        self.model.train()
        batch = next(iter(self.dataloader))
        
        with torch.no_grad():
            outputs = self.model(**batch)
            loss = outputs.loss
            
            print(f"Loss: {loss.item():.6f}")
            
            # æ£€æŸ¥è¾“å‡ºåˆ†å¸ƒ
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)
            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
            print(f"Output entropy: {entropy.item():.6f}")
        
        # æ¢¯åº¦æ£€æŸ¥
        loss.backward()
        
        total_norm = 0
        param_count = 0
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
                
                # æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
                if torch.isnan(param.grad).any():
                    print(f"WARNING: NaN gradients in {name}")
                if param_norm > 10.0:
                    print(f"WARNING: Large gradients in {name}: {param_norm:.6f}")
        
        total_norm = total_norm ** (1. / 2)
        print(f"Total gradient norm: {total_norm:.6f}")
    
    def generate_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        report = {
            'data_issues': [],
            'model_issues': [],
            'training_issues': [],
            'recommendations': []
        }
        
        # åŸºäºæ£€æŸ¥ç»“æœç”Ÿæˆå»ºè®®
        if self.metrics.get('gradient_norm', 0) > 5.0:
            report['training_issues'].append('Large gradient norm detected')
            report['recommendations'].append('Consider gradient clipping')
        
        if self.metrics.get('loss', float('inf')) > 10.0:
            report['training_issues'].append('High initial loss')
            report['recommendations'].append('Check label smoothing and learning rate')
        
        return report

# å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ
common_issues = {
    'loss_not_decreasing': {
        'symptoms': ['Loss stays constant', 'Very slow convergence'],
        'causes': ['Learning rate too small', 'Gradient vanishing', 'Data issues'],
        'solutions': [
            'Increase learning rate',
            'Check gradient flow',
            'Verify data preprocessing',
            'Add gradient clipping'
        ]
    },
    
    'loss_exploding': {
        'symptoms': ['Loss becomes NaN', 'Very large gradients'],
        'causes': ['Learning rate too large', 'Gradient explosion', 'Numerical instability'],
        'solutions': [
            'Decrease learning rate',
            'Add gradient clipping',
            'Use mixed precision training',
            'Check weight initialization'
        ]
    },
    
    'overfitting': {
        'symptoms': ['Train loss << Val loss', 'Perfect train accuracy'],
        'causes': ['Model too complex', 'Insufficient regularization', 'Small dataset'],
        'solutions': [
            'Add dropout',
            'Increase weight decay',
            'Use data augmentation',
            'Reduce model size'
        ]
    }
}
```

---

### Q10: æ€§èƒ½ç›‘æ§ä¸åˆ†æ
**é—®é¢˜ï¼š** å¦‚ä½•è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„Transformerè®­ç»ƒç›‘æ§ç³»ç»Ÿï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- ç³»ç»Ÿè®¾è®¡èƒ½åŠ›
- æ€§èƒ½åˆ†ææ€ç»´
- å·¥ç¨‹å®è·µç»éªŒ

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# å®Œæ•´ç›‘æ§ç³»ç»Ÿè®¾è®¡

class TransformerMonitor:
    def __init__(self, model, log_dir='./logs'):
        self.model = model
        self.log_dir = log_dir
        self.metrics_history = defaultdict(list)
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        from torch.utils.tensorboard import SummaryWriter
        import wandb
        
        # TensorBoard
        self.tb_writer = SummaryWriter(self.log_dir)
        
        # Weights & Biases
        wandb.init(project="transformer-training")
        
        # è‡ªå®šä¹‰æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'{self.log_dir}/training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_training_step(self, step, loss, lr, batch_time, **kwargs):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        
        # åŸºç¡€æŒ‡æ ‡
        metrics = {
            'train/loss': loss,
            'train/learning_rate': lr,
            'train/batch_time': batch_time,
            'train/tokens_per_second': kwargs.get('tokens_per_second', 0)
        }
        
        # æ¨¡å‹æŒ‡æ ‡
        if step % 100 == 0:
            metrics.update(self.compute_model_metrics())
        
        # ç³»ç»ŸæŒ‡æ ‡
        metrics.update(self.compute_system_metrics())
        
        # è®°å½•åˆ°å„ä¸ªç³»ç»Ÿ
        for key, value in metrics.items():
            self.tb_writer.add_scalar(key, value, step)
            wandb.log({key: value}, step=step)
            self.metrics_history[key].append((step, value))
        
        # æ§åˆ¶å°è¾“å‡º
        if step % 10 == 0:
            self.logger.info(
                f"Step {step}: Loss={loss:.4f}, LR={lr:.2e}, "
                f"Time={batch_time:.2f}s, GPU={self.get_gpu_usage():.1f}%"
            )
    
    def compute_model_metrics(self):
        """è®¡ç®—æ¨¡å‹ç›¸å…³æŒ‡æ ‡"""
        metrics = {}
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        metrics['model/total_parameters'] = total_params
        metrics['model/trainable_parameters'] = trainable_params
        
        # æ¢¯åº¦ç»Ÿè®¡
        total_norm = 0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        
        metrics['model/gradient_norm'] = total_norm ** 0.5
        
        # æƒé‡ç»Ÿè®¡
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                metrics[f'weights/{name}_mean'] = param.data.mean().item()
                metrics[f'weights/{name}_std'] = param.data.std().item()
        
        return metrics
    
    def compute_system_metrics(self):
        """è®¡ç®—ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
        import psutil
        
        metrics = {
            'system/cpu_percent': psutil.cpu_percent(),
            'system/memory_percent': psutil.virtual_memory().percent,
            'system/gpu_memory_used': torch.cuda.memory_allocated() / 1024**3,  # GB
            'system/gpu_memory_cached': torch.cuda.memory_reserved() / 1024**3,  # GB
        }
        
        return metrics
    
    def get_gpu_usage(self):
        """è·å–GPUä½¿ç”¨ç‡"""
        try:
            import nvidia_ml_py3 as nvml
            nvml.nvmlInit()
            handle = nvml.nvmlDeviceGetHandleByIndex(0)
            util = nvml.nvmlDeviceGetUtilizationRates(handle)
            return util.gpu
        except:
            return 0.0
    
    def log_validation(self, step, val_metrics):
        """è®°å½•éªŒè¯æŒ‡æ ‡"""
        for key, value in val_metrics.items():
            self.tb_writer.add_scalar(f'val/{key}', value, step)
            wandb.log({f'val/{key}': value}, step=step)
        
        self.logger.info(f"Validation at step {step}: {val_metrics}")
    
    def generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'training_summary': {
                'total_steps': len(self.metrics_history['train/loss']),
                'final_loss': self.metrics_history['train/loss'][-1][1],
                'best_val_loss': min([v for _, v in self.metrics_history.get('val/loss', [(0, float('inf'))])]),
            },
            'performance_analysis': self.analyze_performance(),
            'resource_usage': self.analyze_resource_usage(),
            'recommendations': self.generate_recommendations()
        }
        
        return report
    
    def analyze_performance(self):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        loss_history = [v for _, v in self.metrics_history['train/loss']]
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦
        if len(loss_history) > 100:
            early_loss = np.mean(loss_history[:50])
            recent_loss = np.mean(loss_history[-50:])
            improvement_rate = (early_loss - recent_loss) / early_loss
        else:
            improvement_rate = 0
        
        return {
            'convergence_rate': improvement_rate,
            'loss_stability': np.std(loss_history[-100:]) if len(loss_history) > 100 else 0,
            'training_efficiency': self.compute_training_efficiency()
        }
    
    def compute_training_efficiency(self):
        """è®¡ç®—è®­ç»ƒæ•ˆç‡"""
        if 'train/tokens_per_second' in self.metrics_history:
            tps_history = [v for _, v in self.metrics_history['train/tokens_per_second']]
            return {
                'avg_tokens_per_second': np.mean(tps_history),
                'peak_tokens_per_second': np.max(tps_history)
            }
        return {}
    
    def analyze_resource_usage(self):
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        gpu_memory = [v for _, v in self.metrics_history.get('system/gpu_memory_used', [])]
        
        return {
            'peak_gpu_memory': max(gpu_memory) if gpu_memory else 0,
            'avg_gpu_memory': np.mean(gpu_memory) if gpu_memory else 0,
            'memory_efficiency': self.compute_memory_efficiency()
        }
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ€§èƒ½åˆ†æç”Ÿæˆå»ºè®®
        performance = self.analyze_performance()
        
        if performance['convergence_rate'] < 0.1:
            recommendations.append("Consider increasing learning rate or adjusting scheduler")
        
        if performance['loss_stability'] > 0.1:
            recommendations.append("Training appears unstable, consider gradient clipping")
        
        # åŸºäºèµ„æºä½¿ç”¨ç”Ÿæˆå»ºè®®
        resource = self.analyze_resource_usage()
        
        if resource['peak_gpu_memory'] > 0.9:
            recommendations.append("GPU memory usage is high, consider reducing batch size")
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
monitor = TransformerMonitor(model)

for step, batch in enumerate(dataloader):
    start_time = time.time()
    
    # è®­ç»ƒæ­¥éª¤
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    
    # è®°å½•æŒ‡æ ‡
    batch_time = time.time() - start_time
    monitor.log_training_step(
        step=step,
        loss=loss.item(),
        lr=optimizer.param_groups[0]['lr'],
        batch_time=batch_time,
        tokens_per_second=batch['input_ids'].numel() / batch_time
    )
    
    # å®šæœŸéªŒè¯
    if step % 1000 == 0:
        val_metrics = evaluate_model(model, val_dataloader)
        monitor.log_validation(step, val_metrics)

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
final_report = monitor.generate_training_report()
print(json.dumps(final_report, indent=2))
```

---

## é«˜çº§åº”ç”¨ç±»é—®é¢˜

### Q11: å¤šä»»åŠ¡å­¦ä¹ å®ç°
**é—®é¢˜ï¼š** å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ çš„Transformeræ¶æ„ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- æ¶æ„è®¾è®¡èƒ½åŠ›
- å¤šä»»åŠ¡å­¦ä¹ ç†è§£
- å‚æ•°å…±äº«ç­–ç•¥

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# å¤šä»»åŠ¡Transformerè®¾è®¡

class MultiTaskTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # å…±äº«ç¼–ç å™¨
        self.shared_encoder = TransformerEncoder(
            num_layers=config.shared_layers,
            d_model=config.d_model,
            num_heads=config.num_heads
        )
        
        # ä»»åŠ¡ç‰¹å®šå±‚
        self.task_specific_layers = nn.ModuleDict()
        self.task_heads = nn.ModuleDict()
        
        for task_name, task_config in config.tasks.items():
            # ä»»åŠ¡ç‰¹å®šçš„Transformerå±‚
            self.task_specific_layers[task_name] = TransformerEncoder(
                num_layers=task_config.specific_layers,
                d_model=config.d_model,
                num_heads=config.num_heads
            )
            
            # ä»»åŠ¡ç‰¹å®šçš„è¾“å‡ºå¤´
            if task_config.task_type == 'classification':
                self.task_heads[task_name] = nn.Linear(
                    config.d_model, task_config.num_classes
                )
            elif task_config.task_type == 'generation':
                self.task_heads[task_name] = nn.Linear(
                    config.d_model, task_config.vocab_size
                )
    
    def forward(self, input_ids, task_name, **kwargs):
        # å…±äº«ç¼–ç 
        shared_output = self.shared_encoder(input_ids)
        
        # ä»»åŠ¡ç‰¹å®šå¤„ç†
        task_output = self.task_specific_layers[task_name](shared_output)
        
        # ä»»åŠ¡ç‰¹å®šé¢„æµ‹
        predictions = self.task_heads[task_name](task_output)
        
        return predictions

# å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥
class MultiTaskTrainer:
    def __init__(self, model, tasks_config):
        self.model = model
        self.tasks_config = tasks_config
        self.task_weights = {name: config.weight for name, config in tasks_config.items()}
        
    def train_step(self, batch_dict):
        total_loss = 0
        task_losses = {}
        
        for task_name, batch in batch_dict.items():
            # å‰å‘ä¼ æ’­
            outputs = self.model(batch['input_ids'], task_name)
            
            # è®¡ç®—ä»»åŠ¡ç‰¹å®šæŸå¤±
            if self.tasks_config[task_name].task_type == 'classification':
                loss = F.cross_entropy(outputs, batch['labels'])
            elif self.tasks_config[task_name].task_type == 'generation':
                loss = F.cross_entropy(
                    outputs.view(-1, outputs.size(-1)),
                    batch['labels'].view(-1)
                )
            
            # åŠ æƒæŸå¤±
            weighted_loss = loss * self.task_weights[task_name]
            total_loss += weighted_loss
            task_losses[task_name] = loss.item()
        
        return total_loss, task_losses
    
    def adaptive_task_weighting(self, task_losses_history):
        """è‡ªé€‚åº”ä»»åŠ¡æƒé‡è°ƒæ•´"""
        # åŸºäºä»»åŠ¡æŸå¤±çš„ç›¸å¯¹å˜åŒ–è°ƒæ•´æƒé‡
        for task_name in self.task_weights:
            recent_losses = task_losses_history[task_name][-10:]
            if len(recent_losses) > 5:
                loss_trend = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]
                
                # å¦‚æœæŸå¤±ä¸‹é™ç¼“æ…¢ï¼Œå¢åŠ æƒé‡
                if loss_trend > -0.01:
                    self.task_weights[task_name] *= 1.1
                else:
                    self.task_weights[task_name] *= 0.95
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.task_weights.values())
        for task_name in self.task_weights:
            self.task_weights[task_name] /= total_weight

# å‚æ•°å…±äº«ç­–ç•¥
sharing_strategies = {
    'full_sharing': {
        'description': 'æ‰€æœ‰ä»»åŠ¡å…±äº«å…¨éƒ¨å‚æ•°',
        'pros': ['å‚æ•°æ•ˆç‡é«˜', 'ä»»åŠ¡é—´çŸ¥è¯†è¿ç§»'],
        'cons': ['ä»»åŠ¡å†²çª', 'æ€§èƒ½å¯èƒ½å—é™']
    },
    
    'partial_sharing': {
        'description': 'åº•å±‚å…±äº«ï¼Œé¡¶å±‚ä»»åŠ¡ç‰¹å®š',
        'pros': ['å¹³è¡¡å…±äº«å’Œç‰¹åŒ–', 'è¾ƒå¥½çš„æ€§èƒ½'],
        'cons': ['å‚æ•°å¢åŠ ', 'è®¾è®¡å¤æ‚']
    },
    
    'adapter_based': {
        'description': 'åœ¨å…±äº«å±‚ä¸­æ’å…¥ä»»åŠ¡ç‰¹å®šé€‚é…å™¨',
        'pros': ['å‚æ•°æ•ˆç‡', 'çµæ´»æ€§é«˜'],
        'cons': ['è®­ç»ƒå¤æ‚', 'æ¨ç†å¼€é”€']
    }
}
```

---

### Q12: æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²
**é—®é¢˜ï¼š** å¦‚ä½•å°†è®­ç»ƒå¥½çš„å¤§å‹Transformeræ¨¡å‹å‹ç¼©å¹¶éƒ¨ç½²åˆ°èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Ÿ

**è€ƒæŸ¥ç‚¹ï¼š**
- æ¨¡å‹å‹ç¼©æŠ€æœ¯
- éƒ¨ç½²ä¼˜åŒ–ç­–ç•¥
- æ€§èƒ½ä¸ç²¾åº¦æƒè¡¡

**å‚è€ƒç­”æ¡ˆè¦ç‚¹ï¼š**
```python
# ç»¼åˆæ¨¡å‹å‹ç¼©æ–¹æ¡ˆ

class TransformerCompressor:
    def __init__(self, model, compression_config):
        self.model = model
        self.config = compression_config
        self.compressed_model = None
    
    def compress(self):
        """æ‰§è¡Œæ¨¡å‹å‹ç¼©"""
        compressed_model = copy.deepcopy(self.model)
        
        # 1. çŸ¥è¯†è’¸é¦
        if self.config.use_distillation:
            compressed_model = self.knowledge_distillation(compressed_model)
        
        # 2. æ¨¡å‹å‰ªæ
        if self.config.use_pruning:
            compressed_model = self.structured_pruning(compressed_model)
        
        # 3. é‡åŒ–
        if self.config.use_quantization:
            compressed_model = self.quantization(compressed_model)
        
        # 4. æƒé‡å…±äº«
        if self.config.use_weight_sharing:
            compressed_model = self.weight_sharing(compressed_model)
        
        self.compressed_model = compressed_model
        return compressed_model
    
    def knowledge_distillation(self, student_model):
        """çŸ¥è¯†è’¸é¦å‹ç¼©"""
        teacher_model = self.model
        teacher_model.eval()
        
        # åˆ›å»ºæ›´å°çš„å­¦ç”Ÿæ¨¡å‹
        student_config = copy.deepcopy(self.model.config)
        student_config.num_layers = self.config.student_layers
        student_config.d_model = self.config.student_d_model
        
        student = TransformerModel(student_config)
        
        # è’¸é¦è®­ç»ƒ
        distillation_loss = DistillationLoss(
            temperature=self.config.temperature,
            alpha=self.config.alpha
        )
        
        optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)
        
        for batch in self.config.distillation_dataloader:
            with torch.no_grad():
                teacher_outputs = teacher_model(**batch)
            
            student_outputs = student(**batch)
            
            loss = distillation_loss(
                student_outputs.logits,
                teacher_outputs.logits,
                batch['labels']
            )
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        
        return student
    
    def structured_pruning(self, model):
        """ç»“æ„åŒ–å‰ªæ"""
        # è®¡ç®—æ¯å±‚çš„é‡è¦æ€§åˆ†æ•°
        importance_scores = self.compute_layer_importance(model)
        
        # é€‰æ‹©è¦ä¿ç•™çš„å±‚
        num_layers_to_keep = int(len(importance_scores) * self.config.pruning_ratio)
        layers_to_keep = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)[:num_layers_to_keep]
        
        # åˆ›å»ºå‰ªæåçš„æ¨¡å‹
        pruned_config = copy.deepcopy(model.config)
        pruned_config.num_layers = num_layers_to_keep
        
        pruned_model = TransformerModel(pruned_config)
        
        # å¤åˆ¶ä¿ç•™å±‚çš„æƒé‡
        kept_layer_indices = [idx for idx, _ in layers_to_keep]
        for i, layer_idx in enumerate(kept_layer_indices):
            pruned_model.transformer.layers[i].load_state_dict(
                model.transformer.layers[layer_idx].state_dict()
            )
        
        return pruned_model
    
    def compute_layer_importance(self, model):
        """è®¡ç®—å±‚é‡è¦æ€§åˆ†æ•°"""
        importance_scores = {}
        
        # åŸºäºæ¢¯åº¦çš„é‡è¦æ€§è¯„ä¼°
        model.train()
        for i, layer in enumerate(model.transformer.layers):
            layer_gradients = []
            
            for batch in self.config.importance_dataloader:
                outputs = model(**batch)
                loss = outputs.loss
                loss.backward(retain_graph=True)
                
                # æ”¶é›†è¯¥å±‚çš„æ¢¯åº¦
                layer_grad_norm = 0
                for param in layer.parameters():
                    if param.grad is not None:
                        layer_grad_norm += param.grad.norm().item()
                
                layer_gradients.append(layer_grad_norm)
                model.zero_grad()
            
            importance_scores[i] = np.mean(layer_gradients)
        
        return importance_scores
    
    def quantization(self, model):
        """æ¨¡å‹é‡åŒ–"""
        if self.config.quantization_type == 'dynamic':
            # åŠ¨æ€é‡åŒ–
            quantized_model = torch.quantization.quantize_dynamic(
                model, {nn.Linear}, dtype=torch.qint8
            )
        
        elif self.config.quantization_type == 'static':
            # é™æ€é‡åŒ–
            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
            model_prepared = torch.quantization.prepare(model)
            
            # æ ¡å‡†
            model_prepared.eval()
            with torch.no_grad():
                for batch in self.config.calibration_dataloader:
                    model_prepared(**batch)
            
            quantized_model = torch.quantization.convert(model_prepared)
        
        return quantized_model
    
    def weight_sharing(self, model):
        """æƒé‡å…±äº«"""
        # å¯¹ç›¸ä¼¼çš„æƒé‡çŸ©é˜µè¿›è¡Œèšç±»å’Œå…±äº«
        from sklearn.cluster import KMeans
        
        for name, param in model.named_parameters():
            if 'weight' in name and param.dim() == 2:
                # å°†æƒé‡çŸ©é˜µå±•å¹³
                weights_flat = param.data.view(-1)
                
                # K-meansèšç±»
                kmeans = KMeans(n_clusters=self.config.num_clusters)
                clusters = kmeans.fit_predict(weights_flat.cpu().numpy().reshape(-1, 1))
                
                # ç”¨èšç±»ä¸­å¿ƒæ›¿æ¢åŸæƒé‡
                for i in range(self.config.num_clusters):
                    mask = clusters == i
                    weights_flat[mask] = kmeans.cluster_centers_[i]
                
                param.data = weights_flat.view(param.shape)
        
        return model
    
    def evaluate_compression(self):
        """è¯„ä¼°å‹ç¼©æ•ˆæœ"""
        if self.compressed_model is None:
            raise ValueError("Model not compressed yet")
        
        # æ¨¡å‹å¤§å°å¯¹æ¯”
        original_size = sum(p.numel() for p in self.model.parameters())
        compressed_size = sum(p.numel() for p in self.compressed_model.parameters())
        compression_ratio = original_size / compressed_size
        
        # æ€§èƒ½å¯¹æ¯”
        original_metrics = self.evaluate_model(self.model)
        compressed_metrics = self.evaluate_model(self.compressed_model)
        
        # æ¨ç†é€Ÿåº¦å¯¹æ¯”
        original_speed = self.benchmark_inference(self.model)
        compressed_speed = self.benchmark_inference(self.compressed_model)
        
        return {
            'compression_ratio': compression_ratio,
            'size_reduction': f"{(1 - 1/compression_ratio)*100:.1f}%",
            'performance_drop': {
                metric: f"{(original_metrics[metric] - compressed_metrics[metric])/original_metrics[metric]*100:.1f}%"
                for metric in original_metrics
            },
            'speedup': compressed_speed / original_speed
        }

# éƒ¨ç½²ä¼˜åŒ–
class TransformerDeployment:
    def __init__(self, model, deployment_config):
        self.model = model
        self.config = deployment_config
    
    def optimize_for_deployment(self):
        """éƒ¨ç½²ä¼˜åŒ–"""
        optimized_model = copy.deepcopy(self.model)
        
        # 1. å›¾ä¼˜åŒ–
        if self.config.use_torchscript:
            optimized_model = self.torchscript_optimization(optimized_model)
        
        # 2. ONNXå¯¼å‡º
        if self.config.export_onnx:
            self.export_to_onnx(optimized_model)
        
        # 3. TensorRTä¼˜åŒ–
        if self.config.use_tensorrt:
            optimized_model = self.tensorrt_optimization(optimized_model)
        
        return optimized_model
    
    def torchscript_optimization(self, model):
        """TorchScriptä¼˜åŒ–"""
        model.eval()
        
        # è¿½è¸ªæ¨¡å‹
        dummy_input = torch.randint(0, 1000, (1, 512))
        traced_model = torch.jit.trace(model, dummy_input)
        
        # ä¼˜åŒ–
        optimized_model = torch.jit.optimize_for_inference(traced_model)
        
        return optimized_model
    
    def export_to_onnx(self, model):
        """å¯¼å‡ºONNXæ ¼å¼"""
        dummy_input = torch.randint(0, 1000, (1, 512))
        
        torch.onnx.export(
            model,
            dummy_input,
            self.config.onnx_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input_ids'],
            output_names=['logits'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'logits': {0: 'batch_size', 1: 'sequence'}
            }
        )
    
    def create_serving_api(self):
        """åˆ›å»ºæœåŠ¡API"""
        from flask import Flask, request, jsonify
        
        app = Flask(__name__)
        
        @app.route('/predict', methods=['POST'])
        def predict():
            data = request.json
            input_text = data['text']
            
            # é¢„å¤„ç†
            inputs = self.preprocess(input_text)
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # åå¤„ç†
            results = self.postprocess(outputs)
            
            return jsonify(results)
        
        @app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy'})
        
        return app

# ä½¿ç”¨ç¤ºä¾‹
compression_config = {
    'use_distillation': True,
    'student_layers': 6,
    'student_d_model': 384,
    'temperature': 3.0,
    'alpha': 0.7,
    'use_pruning': True,
    'pruning_ratio': 0.8,
    'use_quantization': True,
    'quantization_type': 'dynamic'
}

compressor = TransformerCompressor(model, compression_config)
compressed_model = compressor.compress()
evaluation_results = compressor.evaluate_compression()

print(f"Compression ratio: {evaluation_results['compression_ratio']:.2f}x")
print(f"Size reduction: {evaluation_results['size_reduction']}")
print(f"Speedup: {evaluation_results['speedup']:.2f}x")
```

---

## é¢è¯•æŠ€å·§ä¸æ€»ç»“

### ğŸ¯ å›ç­”ç­–ç•¥

1. **åˆ†å±‚å›ç­”**ï¼š
   - å…ˆå›ç­”æ ¸å¿ƒæ¦‚å¿µ
   - å†è¯¦è¿°æŠ€æœ¯ç»†èŠ‚
   - æœ€åè®¨è®ºå®é™…åº”ç”¨

2. **ä»£ç å¯¼å‘**ï¼š
   - ç”¨ä»£ç ç‰‡æ®µæ”¯æ’‘ç†è®ºè§£é‡Š
   - å±•ç¤ºå®é™…å®ç°èƒ½åŠ›
   - ä½“ç°å·¥ç¨‹æ€ç»´

3. **é—®é¢˜é©±åŠ¨**ï¼š
   - ä¸»åŠ¨æå‡ºç›¸å…³é—®é¢˜
   - å±•ç¤ºæ·±åº¦æ€è€ƒ
   - å¼•å¯¼é¢è¯•æ–¹å‘

### ğŸš€ åŠ åˆ†æŠ€å·§

1. **å®è·µç»éªŒ**ï¼š
   - åˆ†äº«å®é™…é¡¹ç›®ç»éªŒ
   - è®¨è®ºé‡åˆ°çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ
   - å±•ç¤ºä¼˜åŒ–æ€è·¯

2. **å‰æ²¿äº†è§£**ï¼š
   - å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•
   - äº†è§£ä¸šç•Œæœ€ä½³å®è·µ
   - æ€è€ƒæœªæ¥å‘å±•æ–¹å‘

3. **ç³»ç»Ÿæ€ç»´**ï¼š
   - ä»æ¶æ„è®¾è®¡è§’åº¦æ€è€ƒ
   - è€ƒè™‘å·¥ç¨‹å®è·µçº¦æŸ
   - å¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—

### âš ï¸ å¸¸è§é™·é˜±

1. **ç†è®ºä¸å®è·µè„±èŠ‚**ï¼š
   - åªçŸ¥é“ç†è®ºå…¬å¼ï¼Œä¸äº†è§£å®ç°ç»†èŠ‚
   - å¿½ç•¥å·¥ç¨‹ä¼˜åŒ–çš„é‡è¦æ€§
   - ä¸è€ƒè™‘å®é™…éƒ¨ç½²çº¦æŸ

2. **ç»†èŠ‚é—æ¼**ï¼š
   - å¿½ç•¥æ•°å€¼ç¨³å®šæ€§é—®é¢˜
   - ä¸è€ƒè™‘å†…å­˜å’Œè®¡ç®—å¤æ‚åº¦
   - å¿½ç•¥è¾¹ç•Œæ¡ä»¶å¤„ç†

3. **ç¼ºä¹å®è·µç»éªŒ**ï¼š
   - æ²¡æœ‰å®é™…è°ƒè¯•ç»éªŒ
   - ä¸äº†è§£å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ
   - ç¼ºä¹æ€§èƒ½ä¼˜åŒ–æ„è¯†

### ğŸ“ˆ è¿›é˜¶å­¦ä¹ å»ºè®®

1. **åŠ¨æ‰‹å®è·µ**ï¼š
   - ä»é›¶å®ç°mini-Transformer
   - åœ¨ä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹
   - å°è¯•å„ç§ä¼˜åŒ–æŠ€æœ¯

2. **æºç é˜…è¯»**ï¼š
   - ç ”è¯»Hugging Face Transformers
   - åˆ†æPyTorchå®˜æ–¹å®ç°
   - å­¦ä¹ å·¥ä¸šçº§ä»£ç ç»“æ„

3. **æŒç»­å­¦ä¹ **ï¼š
   - å…³æ³¨æœ€æ–°ç ”ç©¶è®ºæ–‡
   - å‚ä¸å¼€æºé¡¹ç›®
   - åˆ†äº«å­¦ä¹ å¿ƒå¾—

é€šè¿‡è¿™äº›é—®é¢˜çš„æ·±å…¥å‡†å¤‡ï¼Œä½ å°†èƒ½å¤Ÿåœ¨é¢è¯•ä¸­å±•ç°å¯¹Transformerå®ç°çš„å…¨é¢ç†è§£å’Œå®è·µèƒ½åŠ›ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§ä¼˜åŒ–ï¼Œä»ç†è®ºåŸç†åˆ°å·¥ç¨‹å®è·µï¼Œå½¢æˆå®Œæ•´çš„çŸ¥è¯†ä½“ç³»ã€‚