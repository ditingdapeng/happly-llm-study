# Task06：Encoder-only PLM 面试问题集

## 基础概念理解 (初级-中级)

### Q1: 什么是Encoder-only预训练语言模型？它与传统语言模型有什么根本区别？

**核心考察点**：基础概念理解

**参考答案**：
Encoder-only预训练语言模型是基于Transformer Encoder架构的双向语言模型，以BERT为代表。与传统单向语言模型的根本区别在于：

1. **双向性**：能同时利用左右上下文信息，而传统LM只能从左到右
2. **预训练目标**：使用MLM而非下一词预测
3. **应用场景**：专注理解任务而非生成任务
4. **信息流**：允许信息在所有位置间自由流动

**追问**：为什么双向性对理解任务如此重要？

### Q2: 解释BERT的Masked Language Model (MLM)训练策略，为什么不直接预测下一个词？

**核心考察点**：预训练策略理解

**参考答案**：
MLM策略随机掩码15%的输入token，让模型预测被掩码的词。具体策略：
- 80%替换为[MASK]
- 10%替换为随机词
- 10%保持原词不变

不直接预测下一词的原因：
1. **避免信息泄露**：下一词预测会导致模型只关注左侧上下文
2. **双向学习**：MLM迫使模型利用完整上下文
3. **语义理解**：需要深层语义推理而非简单的统计模式

**追问**：为什么要有10%的随机替换和10%的保持不变？

### Q3: Next Sentence Prediction (NSP)任务的设计初衷是什么？为什么后来被认为不重要？

**核心考察点**：任务设计理解和发展历程

**参考答案**：
NSP设计初衷：
- 学习句子间关系
- 为下游任务（如问答、推理）提供句子级理解
- 增强文档级语义表示

被认为不重要的原因：
1. **任务过于简单**：句子连续性容易通过主题一致性判断
2. **RoBERTa实验**：移除NSP后性能反而提升
3. **负面影响**：可能干扰MLM的学习
4. **数据构造问题**：负样本构造不够自然

**深入思考**：这说明了什么关于任务设计的原则？

## 架构深度理解 (中级-高级)

### Q4: BERT为什么选择学习式位置编码而不是固定的正弦位置编码？这个选择有什么影响？

**核心考察点**：架构设计细节

**参考答案**：
选择学习式位置编码的原因：
1. **灵活性**：可以学习任务特定的位置模式
2. **相对位置**：更关注词间相对关系而非绝对位置
3. **适应性**：能适应不同类型的文本结构

影响：
- **优势**：更好的位置表示学习
- **劣势**：限制了序列长度扩展性
- **泛化性**：在不同长度文本上的表现

**追问**：如何解决BERT的序列长度限制问题？

### Q5: 分析BERT中多头注意力的作用，不同的注意力头学习到了什么？

**核心考察点**：注意力机制深度理解

**参考答案**：
研究表明不同注意力头的分工：

1. **语法头**：学习句法关系（主谓宾、修饰关系）
2. **语义头**：捕捉语义相似性和共指关系
3. **位置头**：关注相邻词或固定距离的词
4. **长距离头**：处理长距离依赖关系

**实证方法**：
```python
# 注意力模式分析
attention_weights = model.get_attention_weights()
# 可视化不同头的注意力分布
# 分析高权重的词对关系
```

**追问**：如何设计实验来验证这些发现？

### Q6: 解释BERT的层次化表示学习，不同层学习到了什么特征？

**核心考察点**：深层网络的表示学习

**参考答案**：
BERT的层次化学习模式：

**底层（1-4层）**：
- 词法特征（词性、形态）
- 局部语法模式
- 基础语义特征

**中层（5-8层）**：
- 句法结构
- 语义角色
- 实体识别

**高层（9-12层）**：
- 复杂语义关系
- 推理和常识
- 任务特定特征

**实验证据**：通过探针任务（probing tasks）验证各层的语言学特征

**追问**：这种层次化学习对微调策略有什么启示？

## 预训练与微调实践 (中级-高级)

### Q7: 在微调BERT时，应该如何设置学习率？为什么要比从头训练小？

**核心考察点**：微调策略和实践经验

**参考答案**：
学习率设置原则：

1. **整体策略**：比从头训练小10-100倍
   - 预训练：1e-4 到 1e-3
   - 微调：1e-5 到 5e-5

2. **分层学习率**：
```python
# 不同层使用不同学习率
for name, param in model.named_parameters():
    if 'embeddings' in name:
        param.lr = base_lr * 0.1
    elif 'encoder.layer.0' in name:
        param.lr = base_lr * 0.2
    # ... 逐层递增
```

3. **原因分析**：
   - 避免破坏预训练特征
   - 保持底层通用特征稳定
   - 允许高层适应特定任务

**追问**：如何判断学习率是否合适？

### Q8: 什么情况下应该冻结BERT的某些层？如何决定冻结策略？

**核心考察点**：迁移学习策略

**参考答案**：
冻结策略的决策因素：

**数据量**：
- 数据充足：全参数微调
- 数据稀少：冻结底层，只训练顶层
- 极少数据：只训练分类头

**任务相似性**：
- 与预训练任务相似：冻结更多层
- 差异较大：解冻更多层

**计算资源**：
- 资源充足：渐进式解冻
- 资源有限：固定冻结策略

**实践策略**：
```python
# 渐进式解冻
epoch_1: freeze layers 0-8
epoch_2: freeze layers 0-6  
epoch_3: freeze layers 0-4
epoch_4: full fine-tuning
```

**追问**：如何评估冻结策略的效果？

### Q9: 在领域适应中，如何处理BERT遇到的OOV（Out-of-Vocabulary）问题？

**核心考察点**：领域适应和词汇处理

**参考答案**：
OOV处理策略：

1. **WordPiece分词**：
   - 将未知词分解为子词单元
   - 减少真正的OOV情况
   - 保持语义相关性

2. **词汇扩展**：
```python
# 添加领域特定词汇
tokenizer.add_tokens(['COVID-19', 'mRNA', 'cytokine'])
model.resize_token_embeddings(len(tokenizer))
```

3. **继续预训练**：
   - 在领域数据上继续MLM训练
   - 学习领域特定的词汇和模式
   - 保持原有知识的同时适应新领域

4. **多阶段训练**：
   - 阶段1：领域适应预训练
   - 阶段2：任务特定微调

**追问**：如何评估领域适应的效果？

## 性能优化与工程实践 (高级)

### Q10: 在生产环境中部署BERT时，如何进行模型压缩和加速？

**核心考察点**：模型优化和工程实践

**参考答案**：
模型压缩策略：

1. **知识蒸馏**：
```python
# 教师-学生模型训练
teacher_output = teacher_model(input_ids)
student_output = student_model(input_ids)
loss = distillation_loss(student_output, teacher_output)
```

2. **模型剪枝**：
   - 权重剪枝：移除小权重连接
   - 结构剪枝：移除整个注意力头或层
   - 动态剪枝：运行时自适应剪枝

3. **量化**：
   - INT8量化：减少内存占用
   - 动态量化：推理时量化
   - 量化感知训练：训练时考虑量化误差

4. **架构优化**：
   - DistilBERT：6层版本
   - ALBERT：参数共享
   - TinyBERT：专门的小模型

**性能对比**：
- 原始BERT：110M参数，100%性能
- DistilBERT：66M参数，97%性能
- TinyBERT：14M参数，96%性能

**追问**：如何在压缩率和性能之间找到平衡？

### Q11: 如何处理BERT的长文本限制？有哪些解决方案？

**核心考察点**：长序列处理和架构改进

**参考答案**：
长文本处理策略：

1. **滑动窗口**：
```python
# 重叠窗口处理
window_size = 512
overlap = 128
for i in range(0, len(text), window_size - overlap):
    window = text[i:i + window_size]
    process_window(window)
```

2. **层次化处理**：
   - 句子级编码
   - 文档级聚合
   - 多层次注意力

3. **长序列变体**：
   - **Longformer**：稀疏注意力模式
   - **BigBird**：随机+局部+全局注意力
   - **Linformer**：线性复杂度注意力

4. **分段策略**：
   - 智能分段（按句子、段落）
   - 重要性采样
   - 动态长度调整

**效果对比**：
- BERT：512 tokens，O(n²)复杂度
- Longformer：4096 tokens，O(n)复杂度
- BigBird：4096 tokens，稀疏注意力

**追问**：在什么场景下应该选择哪种方案？

### Q12: 如何设计实验来分析BERT学到了什么语言学知识？

**核心考察点**：模型可解释性和实验设计

**参考答案**：
探针任务（Probing Tasks）设计：

1. **语法探针**：
```python
# 词性标注探针
class POSProbe(nn.Module):
    def __init__(self, hidden_size, num_tags):
        self.classifier = nn.Linear(hidden_size, num_tags)
    
    def forward(self, bert_hidden_states):
        return self.classifier(bert_hidden_states)
```

2. **语义探针**：
   - 命名实体识别
   - 语义角色标注
   - 共指消解

3. **句法探针**：
   - 依存关系解析
   - 成分句法分析
   - 语法一致性

4. **注意力分析**：
```python
# 注意力模式可视化
def analyze_attention_patterns(model, text):
    attention_weights = model.get_attention_weights(text)
    # 分析语法关系的注意力模式
    # 计算注意力与语法树的相关性
    return correlation_score
```

5. **表示空间分析**：
   - t-SNE可视化
   - 聚类分析
   - 线性可分性测试

**实验控制**：
- 对比不同层的表示
- 控制探针复杂度
- 使用多种评估指标

**追问**：如何确保探针任务的有效性？

## 前沿发展与思考 (高级-专家级)

### Q13: 分析RoBERTa相对于BERT的改进，这些改进反映了什么深层问题？

**核心考察点**：模型演进理解和深层思考

**参考答案**：
RoBERTa的关键改进：

1. **移除NSP任务**：
   - 发现：NSP任务可能有害
   - 启示：任务设计需要实证验证
   - 深层问题：如何设计有效的预训练任务？

2. **动态掩码**：
   - 改进：每个epoch使用不同的掩码模式
   - 效果：增加训练数据的多样性
   - 思考：静态掩码是否限制了学习？

3. **更大批次和更多数据**：
   - 发现：计算资源的重要性
   - 启示："更多数据 > 更好算法"
   - 问题：如何在有限资源下优化？

4. **更长的训练**：
   - 观察：BERT可能训练不足
   - 启示：收敛判断的重要性
   - 思考：如何判断最优训练时长？

**深层反思**：
- 模型改进往往来自工程优化而非架构创新
- 实验设计和评估的重要性
- 计算资源对模型性能的决定性作用

**追问**：这些发现对当前的大模型训练有什么启示？

### Q14: ELECTRA的判别式预训练相比MLM有什么优势？为什么更高效？

**核心考察点**：预训练范式创新理解

**参考答案**：
ELECTRA的核心创新：

1. **判别式vs生成式**：
```python
# MLM：预测15%的被掩码词
loss = cross_entropy(predicted_tokens, masked_tokens)  # 只有15%有梯度

# ELECTRA：判断每个词是否被替换
loss = binary_cross_entropy(is_replaced, true_labels)  # 100%有梯度
```

2. **效率优势**：
   - **样本效率**：每个位置都提供训练信号
   - **计算效率**：判别比生成简单
   - **数据效率**：不需要掩码，保持完整输入

3. **生成器-判别器架构**：
   - 小生成器产生替换词
   - 大判别器学习检测
   - 对抗式训练思想

4. **性能提升**：
   - 相同计算量下性能更好
   - 更快的收敛速度
   - 更好的下游任务表现

**理论分析**：
- 为什么判别比生成更适合理解任务？
- 对抗训练在语言模型中的作用
- 密集信号vs稀疏信号的学习效率

**追问**：ELECTRA的思想能否应用到其他模态？

### Q15: 如果让你设计下一代Encoder-only模型，你会考虑哪些改进方向？

**核心考察点**：创新思维和前瞻性思考

**参考答案**：
可能的改进方向：

1. **架构创新**：
   - **混合注意力**：结合局部和全局注意力
   - **动态深度**：根据输入复杂度调整层数
   - **多尺度表示**：不同粒度的特征融合

2. **预训练任务**：
   - **多模态对齐**：文本-图像-音频联合训练
   - **结构化预测**：语法树、知识图谱预测
   - **对比学习**：句子级、文档级对比

3. **效率优化**：
   - **自适应计算**：动态分配计算资源
   - **稀疏激活**：只激活相关的参数
   - **渐进式训练**：从小模型开始逐步扩展

4. **知识整合**：
   - **显式知识注入**：知识图谱、常识库
   - **记忆机制**：外部记忆存储
   - **推理能力**：逻辑推理、因果推理

5. **可解释性**：
   - **内置解释**：模型自带解释机制
   - **可控生成**：可控的表示学习
   - **透明决策**：决策过程可视化

**设计原则**：
- 效率与性能的平衡
- 通用性与专门化的权衡
- 可解释性与复杂性的统一

**追问**：这些改进中哪个最有前景？为什么？

## 面试技巧与总结

### 回答策略

1. **结构化回答**：
   - 先概述核心观点
   - 分点详细阐述
   - 总结关键洞察

2. **理论联系实际**：
   - 结合具体应用场景
   - 提及实际项目经验
   - 讨论工程实践问题

3. **展示深度思考**：
   - 分析设计选择的权衡
   - 讨论局限性和改进方向
   - 连接相关技术发展

4. **保持谦逊和好奇**：
   - 承认知识边界
   - 表达学习意愿
   - 提出反思性问题

### 常见陷阱

1. **过度技术化**：避免只谈技术细节，忽略应用价值
2. **缺乏实践**：理论知识丰富但缺乏实际经验
3. **固化思维**：只知道标准答案，缺乏独立思考
4. **忽略发展**：不了解最新进展和未来趋势

### 进阶学习建议

1. **动手实践**：
   - 实现简化版BERT
   - 在不同任务上微调模型
   - 分析注意力模式和表示

2. **深入源码**：
   - 阅读Transformers库源码
   - 理解优化技巧
   - 学习工程实践

3. **跟踪前沿**：
   - 关注顶级会议论文
   - 参与开源项目
   - 交流讨论心得

4. **系统思考**：
   - 理解技术发展脉络
   - 思考未来发展方向
   - 培养创新思维

---

**记住**：面试不仅是知识的考察，更是思维方式和学习能力的展现。深度理解Encoder-only模型的设计哲学，能够举一反三，才是真正的掌握。