# Task04：Encoder-Decoder 架构面试问题集

## 基础概念类问题

### Q1: Transformer的Encoder-Decoder架构设计原理
**问题：** 为什么Transformer要采用Encoder-Decoder分离的架构设计？这种设计解决了什么问题？

**考查点：**
- 架构设计理念的理解
- 编码和解码阶段的不同需求
- 信息处理流程的认知

**参考答案要点：**
- 编码阶段需要双向理解，解码阶段需要单向生成
- 不同的注意力机制需求（双向 vs 因果性）
- 并行处理 vs 自回归生成的矛盾
- 模块化设计的优势

---

### Q2: 编码器中的自注意力机制
**问题：** 详细解释Transformer编码器中自注意力机制的工作原理，为什么需要多头注意力？

**考查点：**
- 自注意力计算过程
- Q、K、V矩阵的作用
- 多头注意力的必要性
- 位置编码的作用

**参考答案要点：**
```plainText
1. 自注意力计算：
   - Q×K^T计算相似度
   - Softmax归一化得到权重
   - 权重×V得到加权表示

2. 多头注意力优势：
   - 捕获不同类型的依赖关系
   - 增强模型表达能力
   - 避免单一注意力的局限性

3. 位置编码必要性：
   - 自注意力天然缺乏位置信息
   - 正弦位置编码提供相对位置感知
```

---

### Q3: 解码器的三层注意力机制
**问题：** Transformer解码器包含哪三种注意力机制？每种机制的作用是什么？

**考查点：**
- 掩码自注意力的因果性约束
- 交叉注意力的信息交互
- 前馈网络的非线性变换

**参考答案要点：**
```plainText
1. 掩码自注意力（Masked Self-Attention）：
   - 保证因果性约束
   - 只能看到当前位置之前的信息
   - 通过注意力掩码实现

2. 交叉注意力（Cross-Attention）：
   - Q来自解码器，K、V来自编码器
   - 实现编码器-解码器信息交互
   - 动态选择相关的源序列信息

3. 前馈网络（Feed-Forward）：
   - 位置独立的非线性变换
   - 增加模型表达能力
   - 通常是d_model → 4*d_model → d_model
```

---

## 深入理解类问题

### Q4: 训练与推理的不同模式
**问题：** 为什么Transformer在训练时可以并行处理，但在推理时必须串行生成？

**考查点：**
- Teacher Forcing训练策略
- 自回归生成的特性
- 并行与串行的权衡

**参考答案要点：**
```plainText
训练阶段（Teacher Forcing）：
- 目标序列已知
- 可以并行计算所有位置的损失
- 掩码确保因果性约束
- 训练效率高

推理阶段（自回归生成）：
- 下一个词依赖前面所有词
- 必须逐步生成
- 无法预知未来的词
- 推理速度相对较慢
```

---

### Q5: 位置编码的设计选择
**问题：** 为什么Transformer使用正弦位置编码而不是学习式位置编码？各有什么优缺点？

**考查点：**
- 不同位置编码方案的理解
- 设计权衡的考虑
- 泛化能力的分析

**参考答案要点：**
```plainText
正弦位置编码优势：
- 可以处理任意长度序列
- 提供相对位置信息
- 不需要额外参数
- 数学性质良好

学习式位置编码：
- 可以学习任务特定的位置模式
- 但受限于训练时的最大长度
- 需要额外参数
- 泛化能力相对较弱
```

---

### Q6: 注意力掩码的实现细节
**问题：** 详细解释Transformer中注意力掩码的实现原理，为什么要加上一个很大的负数？

**考查点：**
- 掩码机制的技术实现
- Softmax函数的数学特性
- 数值稳定性考虑

**参考答案要点：**
```plainText
实现原理：
1. 创建掩码矩阵（0表示可见，1表示不可见）
2. 将掩码位置的注意力分数设为-∞（实际是-1e9）
3. 经过Softmax后，这些位置的权重接近0

为什么用大负数：
- Softmax(x) = e^x / Σe^x
- 当x→-∞时，e^x→0
- 确保被掩码位置的注意力权重为0
- 避免信息泄露
```

---

## 应用实践类问题

### Q7: 不同任务的架构选择
**问题：** 对于文本分类、文本生成、机器翻译这三个任务，应该选择什么样的Transformer架构？为什么？

**考查点：**
- 任务特性的分析
- 架构适配的理解
- 实际应用的考虑

**参考答案要点：**
```plainText
文本分类：
- 仅编码器架构（如BERT）
- 需要双向上下文理解
- 输出固定维度的分类结果

文本生成：
- 仅解码器架构（如GPT）
- 自回归生成特性
- 因果注意力机制

机器翻译：
- 完整编码器-解码器架构
- 序列到序列转换
- 需要源语言理解和目标语言生成
```

---

### Q8: 模型优化策略
**问题：** 在实际部署Transformer模型时，如何优化推理速度？有哪些常用的加速技术？

**考查点：**
- 推理优化的理解
- 工程实践的经验
- 性能与精度的权衡

**参考答案要点：**
```plainText
常用加速技术：
1. 模型压缩：
   - 知识蒸馏
   - 模型剪枝
   - 量化技术

2. 推理优化：
   - KV缓存
   - 批处理优化
   - 并行计算

3. 架构改进：
   - 线性注意力
   - 稀疏注意力
   - 局部注意力窗口
```

---

## 高级进阶类问题

### Q9: 长序列处理的挑战
**问题：** Transformer在处理长序列时面临什么挑战？有哪些解决方案？

**考查点：**
- 计算复杂度的理解
- 内存限制的认知
- 前沿技术的了解

**参考答案要点：**
```plainText
主要挑战：
1. 计算复杂度：O(n²)的注意力计算
2. 内存消耗：注意力矩阵的存储
3. 位置编码：超出训练长度的泛化

解决方案：
1. 稀疏注意力：Longformer、BigBird
2. 线性注意力：Linformer、Performer
3. 分层处理：Hierarchical Transformer
4. 滑动窗口：局部注意力机制
```

---

### Q10: 注意力机制的可解释性
**问题：** 如何分析和解释Transformer中注意力机制学到的模式？这对模型调试有什么帮助？

**考查点：**
- 模型可解释性的理解
- 注意力可视化的应用
- 调试技巧的掌握

**参考答案要点：**
```plainText
分析方法：
1. 注意力权重可视化
2. 注意力头功能分析
3. 层级注意力模式研究

应用价值：
1. 理解模型决策过程
2. 发现模型偏见和错误
3. 指导模型改进方向
4. 增强模型可信度

常用工具：
- BertViz
- Attention Visualizer
- 自定义可视化脚本
```

---

## 编程实现类问题

### Q11: 手写注意力机制
**问题：** 请用Python实现一个简化版的多头自注意力机制。

**考查点：**
- 代码实现能力
- 算法理解深度
- 工程实践技能

**参考答案框架：**
```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        # 实现多头注意力计算
        pass
```

---

### Q12: 位置编码实现
**问题：** 实现Transformer中的正弦位置编码函数。

**考查点：**
- 数学公式的代码转换
- 张量操作的熟练度
- 边界条件的处理

**参考答案框架：**
```python
def positional_encoding(seq_len, d_model):
    """
    生成正弦位置编码
    """
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1).float()
    
    # 实现正弦和余弦编码
    # PE(pos, 2i) = sin(pos/10000^(2i/d_model))
    # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
    
    return pe
```

---

## 面试技巧提示

### 回答策略
1. **结构化回答**：先概述，再详述，最后总结
2. **举例说明**：用具体例子帮助理解
3. **对比分析**：与其他方法进行比较
4. **实践联系**：结合实际应用场景

### 常见陷阱
1. **概念混淆**：注意区分自注意力和交叉注意力
2. **细节遗漏**：不要忽略掩码、归一化等关键细节
3. **应用局限**：要了解不同架构的适用场景
4. **最新发展**：关注Transformer的最新改进和变体

### 加分项
1. **深入理解**：能够从数学原理解释机制
2. **实践经验**：有实际项目经验和优化心得
3. **前沿了解**：知道最新的研究进展和技术趋势
4. **工程思维**：考虑实际部署中的工程问题

通过这些问题的准备，你将能够在面试中展现对Transformer Encoder-Decoder架构的深入理解和实践能力。