# Task07：Encoder-Decoder PLM 面试问题集

## 基础概念理解 (初级-中级)

### Q1: 什么是Encoder-Decoder预训练语言模型？它与Encoder-only和Decoder-only模型有什么本质区别？

**核心考察点**：架构理解和对比分析

**参考答案**：
Encoder-Decoder预训练语言模型是结合了编码器和解码器的双向架构，以T5、BART为代表。本质区别：

**架构特点**：
- **Encoder**：双向注意力，深度理解输入
- **Decoder**：因果注意力+交叉注意力，条件生成
- **连接**：通过交叉注意力实现信息传递

**能力对比**：
- **Encoder-only (BERT)**：理解强，无生成能力
- **Decoder-only (GPT)**：生成强，理解相对弱
- **Encoder-Decoder (T5)**：理解+生成，适合转换任务

**应用场景**：
- 机器翻译、文本摘要、问答生成
- 任何需要"理解输入，生成输出"的任务

**追问**：为什么不直接用一个大的Decoder-only模型？

### Q2: 解释T5的"Text-to-Text"统一范式，为什么这种设计如此重要？

**核心考察点**：统一框架理解

**参考答案**：
T5将所有NLP任务统一为文本到文本的转换：

**统一格式**：
```
翻译：translate English to German: Hello → Hallo
分类：classify sentiment: I love this → positive
摘要：summarize: [长文本] → [摘要]
问答：question: What is AI? context: [文档] → [答案]
```

**重要性**：
1. **架构统一**：一个模型处理所有任务
2. **训练简化**：统一的损失函数和优化目标
3. **知识共享**：任务间的知识迁移
4. **扩展性强**：新任务只需设计prompt格式

**技术优势**：
- 多任务学习的天然框架
- 零样本和少样本学习能力
- 更好的泛化性能

**局限性**：
- 某些任务可能不适合文本格式
- 输出长度限制
- 计算效率问题

**追问**：这种统一范式对模型规模有什么要求？

### Q3: BART的去噪自编码器预训练策略有哪些？各自的作用是什么？

**核心考察点**：预训练策略理解

**参考答案**：
BART使用多种噪声策略训练去噪能力：

**噪声策略**：
1. **Token Masking**：随机掩码词汇
   - 作用：局部理解和生成
   - 类似BERT的MLM

2. **Token Deletion**：随机删除词汇
   - 作用：信息压缩和恢复
   - 训练模型推断缺失信息

3. **Text Infilling**：用单个mask替换文本片段
   - 作用：长序列生成能力
   - 更接近真实应用场景

4. **Sentence Permutation**：打乱句子顺序
   - 作用：文档结构理解
   - 训练逻辑重组能力

5. **Document Rotation**：旋转文档起始位置
   - 作用：全局一致性
   - 识别文档真实开头

**最佳组合**：
- Text Infilling + Sentence Permutation
- 平衡局部和全局能力

**追问**：为什么BART在摘要任务上表现特别好？

## 架构深度理解 (中级-高级)

### Q4: 详细解释Decoder中交叉注意力的机制，它是如何连接Encoder和Decoder的？

**核心考察点**：注意力机制深度理解

**参考答案**：
交叉注意力是Encoder-Decoder架构的核心连接机制：

**机制详解**：
```python
class CrossAttention(nn.Module):
    def forward(self, decoder_hidden, encoder_output):
        # Query来自Decoder当前状态
        Q = self.query_proj(decoder_hidden)  # [batch, tgt_len, d_model]
        
        # Key和Value来自Encoder输出
        K = self.key_proj(encoder_output)    # [batch, src_len, d_model]
        V = self.value_proj(encoder_output)  # [batch, src_len, d_model]
        
        # 计算注意力权重
        attention_weights = softmax(Q @ K.T / sqrt(d_k))  # [batch, tgt_len, src_len]
        
        # 加权求和
        output = attention_weights @ V  # [batch, tgt_len, d_model]
        return output
```

**信息流动**：
1. **Encoder → Key/Value**：提供源信息
2. **Decoder → Query**：表达信息需求
3. **注意力权重**：动态选择相关信息
4. **输出**：融合的上下文表示

**关键作用**：
- **信息传递**：将源序列信息传递给目标序列
- **动态对齐**：不同位置关注不同的源信息
- **条件生成**：基于源信息的条件生成

**与自注意力的区别**：
- 自注意力：序列内部信息交互
- 交叉注意力：序列间信息交互

**追问**：如何可视化和分析交叉注意力的对齐模式？

### Q5: 为什么Decoder必须使用因果掩码？这种约束带来了什么影响？

**核心考察点**：生成机制理解

**参考答案**：
因果掩码是自回归生成的核心约束：

**必要性分析**：
1. **训练-推理一致性**：
   - 训练时：看到完整目标序列
   - 推理时：只能看到已生成部分
   - 因果掩码保证一致性

2. **避免信息泄露**：
```python
# 没有因果掩码的问题
input: "The cat sat on the"
target: "The cat sat on the mat"
# 如果能看到"mat"，预测"the"后面的词就太简单了

# 因果掩码确保
position_i只能看到position_0到position_{i-1}
```

3. **自回归性质**：
   - 每个位置的生成依赖前面的输出
   - 符合自然语言的生成过程

**实现方式**：
```python
def create_causal_mask(seq_len):
    # 上三角矩阵，True表示masked
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask.bool()

# 在注意力计算中应用
attention_scores = Q @ K.T
attention_scores.masked_fill_(causal_mask, float('-inf'))
attention_weights = softmax(attention_scores)
```

**影响分析**：
- **优势**：保证生成的合理性和一致性
- **劣势**：限制了并行计算能力
- **权衡**：训练时并行，推理时串行

**追问**：有没有可能设计非自回归的生成模型？

### Q6: 分析Teacher Forcing训练和自回归推理的不一致性问题，如何缓解？

**核心考察点**：训练推理差异理解

**参考答案**：
Exposure Bias是Encoder-Decoder模型的经典问题：

**问题分析**：
```python
# 训练时（Teacher Forcing）
for t in range(seq_len):
    # 使用真实的前一个词
    decoder_input[t] = ground_truth[t-1]
    output[t] = decoder(decoder_input[:t+1])

# 推理时（自回归）
for t in range(seq_len):
    # 使用模型预测的前一个词
    decoder_input[t] = model_prediction[t-1]  # 可能有错误！
    output[t] = decoder(decoder_input[:t+1])
```

**问题后果**：
- **错误累积**：早期错误影响后续生成
- **分布偏移**：训练和测试数据分布不同
- **性能下降**：长序列生成质量急剧下降

**缓解策略**：

1. **Scheduled Sampling**：
```python
# 训练时随机选择使用真实词或预测词
if random.random() < teacher_forcing_ratio:
    decoder_input[t] = ground_truth[t-1]  # Teacher forcing
else:
    decoder_input[t] = model_prediction[t-1]  # 自回归

# 逐渐减少teacher_forcing_ratio
teacher_forcing_ratio = max(0.5, 1.0 - epoch * 0.1)
```

2. **Minimum Risk Training**：
```python
# 基于BLEU等指标的风险最小化
loss = -log P(y|x) + λ * BLEU_loss(y_pred, y_true)
```

3. **Self-Critical Training**：
```python
# 使用模型自己的输出作为baseline
reward = BLEU(sample_output, reference) - BLEU(greedy_output, reference)
loss = -reward * log P(sample_output|x)
```

4. **Non-autoregressive Generation**：
   - 并行生成所有位置
   - 避免错误累积
   - 但需要额外的长度预测

**追问**：这些方法的效果如何？有什么新的解决思路？

## 预训练与微调实践 (中级-高级)

### Q7: T5的Span Corruption预训练任务相比BERT的MLM有什么优势？

**核心考察点**：预训练任务设计理解

**参考答案**：
Span Corruption是T5预训练的核心创新：

**任务对比**：
```
BERT MLM:
原文：The cat [MASK] on the [MASK]
预测：sat, mat

T5 Span Corruption:
输入：The cat <X> and <Y>
输出：<X> sat on the mat <Y> looked around
```

**优势分析**：

1. **生成导向**：
   - BERT：分类任务（预测单个词）
   - T5：生成任务（生成连续文本）
   - 更符合下游生成任务

2. **上下文利用**：
   - BERT：15%位置有学习信号
   - T5：所有输出位置都有学习信号
   - 更高的样本效率

3. **长度灵活性**：
   - BERT：固定输入长度
   - T5：可变输入输出长度
   - 更适合实际应用

4. **任务一致性**：
   - 预训练和微调都是seq2seq
   - 减少任务差异

**实现细节**：
```python
def span_corruption(text, corruption_rate=0.15, mean_span_length=3):
    tokens = tokenize(text)
    
    # 选择要破坏的span
    spans_to_corrupt = sample_spans(tokens, corruption_rate, mean_span_length)
    
    # 构造输入和目标
    corrupted_input = []
    target_output = []
    
    sentinel_id = 0
    for span in spans_to_corrupt:
        corrupted_input.append(f"<extra_id_{sentinel_id}>")
        target_output.append(f"<extra_id_{sentinel_id}>")
        target_output.extend(span.tokens)
        sentinel_id += 1
    
    return corrupted_input, target_output
```

**实验结果**：
- 在生成任务上显著优于BERT
- 在理解任务上性能相当
- 统一框架的优势明显

**追问**：如何选择最优的corruption rate和span length？

### Q8: 在微调T5时，如何设计有效的prompt格式？有什么最佳实践？

**核心考察点**：Prompt工程和实践经验

**参考答案**：
Prompt设计是T5微调的关键：

**设计原则**：

1. **任务描述清晰**：
```python
# 好的prompt
"Summarize the following article in 3 sentences: [ARTICLE]"

# 不好的prompt
"Article: [ARTICLE] Summary:"
```

2. **格式一致性**：
```python
# 训练和推理使用相同格式
train_prompt = "translate English to French: {english_text}"
eval_prompt = "translate English to French: {test_text}"
```

3. **任务特定信息**：
```python
# 包含约束条件
"Generate a positive movie review about: {movie_name}"
"Answer the question in one word: {question}"
```

**最佳实践**：

1. **多样化表达**：
```python
prompt_templates = [
    "Translate from English to French: {text}",
    "English to French translation: {text}",
    "Convert to French: {text}",
    "French translation: {text}"
]
```

2. **示例学习**：
```python
# Few-shot prompt
prompt = """
Translate English to French:
English: Hello
French: Bonjour

English: Thank you
French: Merci

English: {input_text}
French:
"""
```

3. **长度控制**：
```python
# 明确输出长度要求
"Summarize in exactly 50 words: {article}"
"Generate a 3-sentence summary: {article}"
```

4. **领域适应**：
```python
# 领域特定prompt
"Medical text summarization: {medical_text}"
"Legal document analysis: {legal_text}"
```

**实验技巧**：
- A/B测试不同prompt格式
- 分析失败案例优化prompt
- 使用验证集选择最佳prompt

**追问**：如何自动化prompt优化过程？

### Q9: 如何处理T5在长文档上的性能问题？有哪些解决方案？

**核心考察点**：长序列处理和优化策略

**参考答案**：
长文档处理是T5的主要挑战：

**问题分析**：
1. **注意力复杂度**：O(n²)随长度平方增长
2. **内存限制**：GPU内存无法容纳长序列
3. **位置编码**：超出预训练长度的位置
4. **质量下降**：长距离依赖建模困难

**解决方案**：

1. **滑动窗口策略**：
```python
def sliding_window_process(long_text, window_size=512, overlap=128):
    results = []
    for i in range(0, len(long_text), window_size - overlap):
        window = long_text[i:i + window_size]
        result = model.generate(window)
        results.append(result)
    
    # 合并结果
    return merge_results(results)
```

2. **层次化处理**：
```python
class HierarchicalT5(nn.Module):
    def __init__(self):
        self.sentence_encoder = T5EncoderModel.from_pretrained('t5-base')
        self.document_encoder = T5EncoderModel.from_pretrained('t5-base')
    
    def forward(self, long_document):
        # 1. 句子级编码
        sentences = split_into_sentences(long_document)
        sentence_embeddings = []
        for sent in sentences:
            emb = self.sentence_encoder(sent).last_hidden_state.mean(dim=1)
            sentence_embeddings.append(emb)
        
        # 2. 文档级编码
        doc_embedding = self.document_encoder(
            torch.stack(sentence_embeddings)
        )
        return doc_embedding
```

3. **稀疏注意力**：
```python
# Longformer风格的注意力模式
class SparseAttention(nn.Module):
    def __init__(self, window_size=256):
        self.window_size = window_size
    
    def create_attention_mask(self, seq_len):
        # 局部注意力窗口
        local_mask = create_local_mask(seq_len, self.window_size)
        
        # 全局注意力位置（如CLS, SEP）
        global_positions = [0, seq_len-1]
        global_mask = create_global_mask(seq_len, global_positions)
        
        return local_mask | global_mask
```

4. **分段摘要**：
```python
def hierarchical_summarization(long_document):
    # 1. 分段
    chunks = split_document(long_document, chunk_size=1000)
    
    # 2. 段落摘要
    chunk_summaries = []
    for chunk in chunks:
        summary = t5_model.generate(
            f"summarize: {chunk}",
            max_length=100
        )
        chunk_summaries.append(summary)
    
    # 3. 最终摘要
    combined_summary = t5_model.generate(
        f"summarize: {' '.join(chunk_summaries)}",
        max_length=200
    )
    
    return combined_summary
```

5. **模型架构改进**：
```python
# LED (Longformer Encoder-Decoder)
from transformers import LEDForConditionalGeneration

model = LEDForConditionalGeneration.from_pretrained(
    'allenai/led-base-16384'
)
# 支持16K长度的输入
```

**性能对比**：
- 滑动窗口：简单但可能丢失全局信息
- 层次化：保持全局视野但增加复杂度
- 稀疏注意力：平衡效率和效果
- LED：专门设计但需要重新训练

**追问**：在什么场景下应该选择哪种方案？

## 性能优化与工程实践 (高级)

### Q10: 如何优化T5模型的推理速度？有哪些工程技巧？

**核心考察点**：模型优化和工程实践

**参考答案**：
T5推理优化是生产部署的关键：

**优化策略**：

1. **模型压缩**：
```python
# 知识蒸馏
class DistillationTrainer:
    def __init__(self, teacher_model, student_model):
        self.teacher = teacher_model
        self.student = student_model
    
    def distillation_loss(self, student_logits, teacher_logits, labels):
        # 软标签损失
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / temperature, dim=-1),
            F.softmax(teacher_logits / temperature, dim=-1),
            reduction='batchmean'
        )
        
        # 硬标签损失
        hard_loss = F.cross_entropy(student_logits, labels)
        
        return alpha * soft_loss + (1 - alpha) * hard_loss
```

2. **量化优化**：
```python
# INT8量化
from transformers import T5ForConditionalGeneration
import torch.quantization as quant

# 动态量化
model = T5ForConditionalGeneration.from_pretrained('t5-base')
quantized_model = quant.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# 静态量化
model.qconfig = quant.get_default_qconfig('fbgemm')
quant.prepare(model, inplace=True)
# 校准数据...
quant.convert(model, inplace=True)
```

3. **缓存优化**：
```python
class CachedT5Generation:
    def __init__(self, model):
        self.model = model
        self.encoder_cache = {}
    
    def generate_with_cache(self, input_text, **kwargs):
        # 缓存encoder输出
        input_hash = hash(input_text)
        if input_hash in self.encoder_cache:
            encoder_outputs = self.encoder_cache[input_hash]
        else:
            encoder_outputs = self.model.encoder(input_ids)
            self.encoder_cache[input_hash] = encoder_outputs
        
        # 只运行decoder
        return self.model.generate(
            encoder_outputs=encoder_outputs,
            **kwargs
        )
```

4. **批处理优化**：
```python
class BatchedInference:
    def __init__(self, model, batch_size=8):
        self.model = model
        self.batch_size = batch_size
        self.pending_requests = []
    
    async def generate(self, input_text):
        # 收集请求
        future = asyncio.Future()
        self.pending_requests.append((input_text, future))
        
        # 批处理
        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()
        
        return await future
    
    async def _process_batch(self):
        batch_inputs = [req[0] for req in self.pending_requests]
        batch_outputs = self.model.generate(batch_inputs)
        
        for (_, future), output in zip(self.pending_requests, batch_outputs):
            future.set_result(output)
        
        self.pending_requests.clear()
```

5. **硬件优化**：
```python
# TensorRT优化
import tensorrt as trt
from torch2trt import torch2trt

# 转换为TensorRT
model_trt = torch2trt(
    model,
    [example_input],
    fp16_mode=True,
    max_workspace_size=1<<30
)

# ONNX导出
torch.onnx.export(
    model,
    example_input,
    "t5_model.onnx",
    opset_version=11,
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence'},
        'output': {0: 'batch_size', 1: 'sequence'}
    }
)
```

**性能提升**：
- 知识蒸馏：50-80%参数减少，5-10%性能损失
- INT8量化：2-4倍速度提升，1-3%性能损失
- 缓存优化：相同输入2-5倍速度提升
- 批处理：吞吐量提升5-10倍
- TensorRT：推理速度提升2-5倍

**追问**：如何在速度和质量之间找到最佳平衡点？

### Q11: 如何设计有效的评估指标来衡量Encoder-Decoder模型的性能？

**核心考察点**：评估方法和指标设计

**参考答案**：
评估Encoder-Decoder模型需要多维度指标：

**传统自动指标**：

1. **BLEU分数**：
```python
from nltk.translate.bleu_score import sentence_bleu

def calculate_bleu(reference, candidate):
    # n-gram精确度的几何平均
    bleu_score = sentence_bleu(
        [reference.split()],
        candidate.split(),
        weights=(0.25, 0.25, 0.25, 0.25)  # 1-4 gram权重
    )
    return bleu_score

# 问题：只考虑精确匹配，忽略语义相似性
```

2. **ROUGE分数**：
```python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(
    ['rouge1', 'rouge2', 'rougeL'],
    use_stemmer=True
)

def calculate_rouge(reference, candidate):
    scores = scorer.score(reference, candidate)
    return {
        'rouge1': scores['rouge1'].fmeasure,
        'rouge2': scores['rouge2'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure
    }
```

**语义相似度指标**：

1. **BERTScore**：
```python
from bert_score import score

def calculate_bertscore(references, candidates):
    P, R, F1 = score(
        candidates,
        references,
        lang='en',
        model_type='bert-base-uncased'
    )
    return {
        'precision': P.mean().item(),
        'recall': R.mean().item(),
        'f1': F1.mean().item()
    }
```

2. **语义相似度**：
```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')

def semantic_similarity(reference, candidate):
    embeddings = model.encode([reference, candidate])
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    return similarity
```

**任务特定指标**：

1. **翻译质量**：
```python
# COMET: 基于预训练模型的评估
from comet import download_model, load_from_checkpoint

model_path = download_model("wmt20-comet-da")
model = load_from_checkpoint(model_path)

def calculate_comet(source, reference, candidate):
    data = [{
        "src": source,
        "mt": candidate,
        "ref": reference
    }]
    scores = model.predict(data)
    return scores[0]
```

2. **摘要质量**：
```python
# 事实一致性检查
class FactualConsistencyChecker:
    def __init__(self):
        self.nli_model = pipeline(
            "text-classification",
            model="microsoft/DialoGPT-medium"
        )
    
    def check_consistency(self, source, summary):
        # 将摘要分解为声明
        claims = extract_claims(summary)
        
        consistency_scores = []
        for claim in claims:
            # 检查每个声明是否与源文档一致
            result = self.nli_model({
                "text": source,
                "text_pair": claim
            })
            consistency_scores.append(result['score'])
        
        return np.mean(consistency_scores)
```

**人工评估框架**：

```python
class HumanEvaluationFramework:
    def __init__(self):
        self.criteria = {
            'fluency': "语言流畅性 (1-5分)",
            'adequacy': "信息充分性 (1-5分)",
            'coherence': "逻辑连贯性 (1-5分)",
            'relevance': "相关性 (1-5分)",
            'creativity': "创新性 (1-5分)"
        }
    
    def evaluate_sample(self, source, reference, candidate):
        scores = {}
        for criterion, description in self.criteria.items():
            score = self.get_human_score(criterion, description, 
                                       source, reference, candidate)
            scores[criterion] = score
        
        return scores
    
    def calculate_agreement(self, evaluator_scores):
        # 计算评估者间一致性
        from scipy.stats import pearsonr
        
        agreements = {}
        for criterion in self.criteria:
            scores = [eval_scores[criterion] for eval_scores in evaluator_scores]
            # 计算Pearson相关系数
            correlation, p_value = pearsonr(scores[0], scores[1])
            agreements[criterion] = correlation
        
        return agreements
```

**综合评估策略**：

```python
class ComprehensiveEvaluator:
    def __init__(self):
        self.auto_metrics = {
            'bleu': calculate_bleu,
            'rouge': calculate_rouge,
            'bertscore': calculate_bertscore,
            'semantic_sim': semantic_similarity
        }
        
        self.human_evaluator = HumanEvaluationFramework()
    
    def evaluate(self, test_data):
        results = {
            'automatic': {},
            'human': {},
            'correlation': {}
        }
        
        # 自动评估
        for metric_name, metric_func in self.auto_metrics.items():
            scores = []
            for sample in test_data:
                score = metric_func(sample['reference'], sample['candidate'])
                scores.append(score)
            results['automatic'][metric_name] = np.mean(scores)
        
        # 人工评估（抽样）
        sample_data = random.sample(test_data, min(100, len(test_data)))
        human_scores = []
        for sample in sample_data:
            score = self.human_evaluator.evaluate_sample(
                sample['source'], sample['reference'], sample['candidate']
            )
            human_scores.append(score)
        
        results['human'] = self.aggregate_human_scores(human_scores)
        
        # 计算自动指标与人工评估的相关性
        results['correlation'] = self.calculate_correlation(
            results['automatic'], results['human']
        )
        
        return results
```

**最佳实践**：
1. **多指标结合**：不依赖单一指标
2. **任务特定**：根据任务选择合适指标
3. **人工验证**：定期进行人工评估
4. **相关性分析**：验证自动指标的有效性
5. **错误分析**：深入分析失败案例

**追问**：如何设计针对特定领域的评估指标？

## 前沿发展与思考 (高级-专家级)

### Q12: 分析UL2统一语言学习框架的创新点，它解决了什么问题？

**核心考察点**：前沿技术理解和分析能力

**参考答案**：
UL2是对预训练范式的重要创新：

**核心创新**：

1. **统一多种预训练目标**：
```python
# UL2的混合目标
pretraining_modes = {
    # R-Denoiser: 常规去噪（类似T5）
    'regular_denoising': {
        'corruption_rate': 0.15,
        'mean_span_length': 3,
        'mode': 'span_corruption'
    },
    
    # S-Denoiser: 顺序去噪（类似GPT）
    'sequential_denoising': {
        'prefix_length': 0.25,  # 25%作为前缀
        'mode': 'prefix_lm'
    },
    
    # X-Denoiser: 极端去噪
    'extreme_denoising': {
        'corruption_rate': 0.5,   # 50%破坏率
        'mean_span_length': 32,   # 长span
        'mode': 'aggressive_span_corruption'
    }
}
```

2. **模式感知训练**：
```python
class UL2Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = TransformerEncoder()
        self.decoder = TransformerDecoder()
        
        # 模式嵌入
        self.mode_embeddings = nn.Embedding(3, hidden_size)  # R, S, X
    
    def forward(self, input_ids, mode_id, **kwargs):
        # 添加模式信息
        mode_emb = self.mode_embeddings(mode_id)
        
        encoder_output = self.encoder(input_ids)
        # 将模式信息注入
        encoder_output = encoder_output + mode_emb
        
        decoder_output = self.decoder(encoder_output, **kwargs)
        return decoder_output
```

**解决的问题**：

1. **单一预训练目标的局限性**：
   - T5的span corruption适合生成，但理解能力有限
   - GPT的自回归适合生成，但双向理解不足
   - UL2结合多种目标的优势

2. **任务适应性**：
```python
# 推理时可以选择合适的模式
def inference_with_mode(model, input_text, task_type):
    if task_type == 'understanding':
        mode_id = 0  # R-Denoiser模式
    elif task_type == 'generation':
        mode_id = 1  # S-Denoiser模式
    elif task_type == 'long_generation':
        mode_id = 2  # X-Denoiser模式
    
    return model.generate(input_text, mode_id=mode_id)
```

3. **规模效应的优化**：
   - 不同模式训练模型的不同能力
   - 避免单一目标的性能瓶颈
   - 更好的参数利用效率

**实验结果**：
- 在理解任务上超越T5
- 在生成任务上接近GPT
- 统一模型的多任务性能优异

**深层意义**：
- 预训练不应该是单一目标
- 模型应该具备多种推理模式
- 统一框架的重要性

**追问**：UL2的思想如何应用到多模态模型？

### Q13: 如何看待Encoder-Decoder架构在大模型时代的地位？它会被Decoder-only取代吗？

**核心考察点**：技术趋势判断和深度思考

**参考答案**：
这是一个深刻的架构哲学问题：

**当前趋势分析**：

1. **Decoder-only的优势**：
   - **统一性**：一个架构处理所有任务
   - **可扩展性**：更容易扩展到超大规模
   - **简洁性**：架构相对简单
   - **涌现能力**：大规模下的涌现现象

2. **Encoder-Decoder的独特价值**：
   - **专门化**：编码和解码的明确分工
   - **效率**：特定任务的计算效率
   - **可控性**：更好的生成控制
   - **可解释性**：清晰的信息流

**深度分析**：

```python
# 不同架构的计算复杂度对比
class ArchitectureComparison:
    def analyze_complexity(self, input_len, output_len):
        # Encoder-Decoder
        encoder_cost = input_len ** 2  # 双向注意力
        decoder_cost = output_len ** 2 + output_len * input_len  # 自注意力+交叉注意力
        ed_total = encoder_cost + decoder_cost
        
        # Decoder-only
        total_len = input_len + output_len
        do_total = total_len ** 2  # 因果注意力
        
        return {
            'encoder_decoder': ed_total,
            'decoder_only': do_total,
            'efficiency_ratio': ed_total / do_total
        }
```

**应用场景分析**：

1. **Encoder-Decoder更适合**：
   - **翻译任务**：明确的源语言和目标语言
   - **摘要任务**：需要深度理解原文
   - **结构化生成**：代码生成、数据转换
   - **条件生成**：基于特定输入的生成

2. **Decoder-only更适合**：
   - **对话系统**：连续的交互
   - **创意写作**：开放式生成
   - **通用助手**：多样化任务
   - **少样本学习**：上下文学习

**未来发展预测**：

1. **混合架构**：
```python
class HybridArchitecture(nn.Module):
    def __init__(self):
        # 根据任务动态选择架构
        self.encoder_decoder = EncoderDecoderModel()
        self.decoder_only = DecoderOnlyModel()
        self.task_router = TaskRouter()
    
    def forward(self, input_text, task_type=None):
        if task_type is None:
            task_type = self.task_router.predict(input_text)
        
        if task_type in ['translation', 'summarization']:
            return self.encoder_decoder(input_text)
        else:
            return self.decoder_only(input_text)
```

2. **架构创新**：
   - **可切换架构**：运行时选择编码解码模式
   - **层次化设计**：不同层使用不同架构
   - **注意力模式混合**：结合双向和单向注意力

**个人观点**：

Encoder-Decoder不会完全被取代，原因：

1. **效率优势**：特定任务的计算效率
2. **专业化需求**：某些应用需要专门设计
3. **可控性要求**：生产环境的可控性需求
4. **资源限制**：不是所有场景都有无限资源

**未来可能的发展**：
- 两种架构并存，各有优势领域
- 出现新的统一架构
- 根据任务自动选择架构的智能系统

**追问**：如何设计一个能够动态切换架构的统一模型？

### Q14: 设计一个理想的多模态Encoder-Decoder模型，你会考虑哪些关键因素？

**核心考察点**：创新设计能力和系统思维

**参考答案**：
设计多模态Encoder-Decoder需要系统性思考：

**核心设计原则**：

1. **模态统一表示**：
```python
class UnifiedModalityEncoder(nn.Module):
    def __init__(self):
        # 不同模态的专门编码器
        self.text_encoder = TextEncoder()
        self.image_encoder = VisionTransformer()
        self.audio_encoder = AudioTransformer()
        self.video_encoder = VideoTransformer()
        
        # 统一投影层
        self.modality_projectors = nn.ModuleDict({
            'text': nn.Linear(text_dim, unified_dim),
            'image': nn.Linear(image_dim, unified_dim),
            'audio': nn.Linear(audio_dim, unified_dim),
            'video': nn.Linear(video_dim, unified_dim)
        })
        
        # 模态融合层
        self.fusion_layer = CrossModalAttention()
    
    def forward(self, inputs):
        modality_embeddings = {}
        
        # 各模态编码
        for modality, data in inputs.items():
            if modality == 'text':
                emb = self.text_encoder(data)
            elif modality == 'image':
                emb = self.image_encoder(data)
            # ... 其他模态
            
            # 投影到统一空间
            unified_emb = self.modality_projectors[modality](emb)
            modality_embeddings[modality] = unified_emb
        
        # 跨模态融合
        fused_representation = self.fusion_layer(modality_embeddings)
        return fused_representation
```

2. **层次化注意力机制**：
```python
class HierarchicalMultiModalAttention(nn.Module):
    def __init__(self):
        # 模态内注意力
        self.intra_modal_attention = nn.ModuleDict({
            modality: SelfAttention() for modality in ['text', 'image', 'audio']
        })
        
        # 模态间注意力
        self.inter_modal_attention = CrossModalAttention()
        
        # 全局注意力
        self.global_attention = GlobalAttention()
    
    def forward(self, modality_inputs):
        # 1. 模态内自注意力
        intra_outputs = {}
        for modality, data in modality_inputs.items():
            intra_outputs[modality] = self.intra_modal_attention[modality](data)
        
        # 2. 模态间交叉注意力
        inter_outputs = {}
        for modality in intra_outputs:
            other_modalities = {k: v for k, v in intra_outputs.items() if k != modality}
            inter_outputs[modality] = self.inter_modal_attention(
                query=intra_outputs[modality],
                key_value_pairs=other_modalities
            )
        
        # 3. 全局融合
        global_output = self.global_attention(inter_outputs)
        return global_output
```

3. **自适应模态权重**：
```python
class AdaptiveModalityWeighting(nn.Module):
    def __init__(self):
        self.modality_importance_predictor = nn.Sequential(
            nn.Linear(unified_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_modalities),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, modality_embeddings, task_context=None):
        # 根据任务和内容动态调整模态权重
        if task_context is not None:
            context_emb = self.encode_task_context(task_context)
            combined_input = torch.cat([modality_embeddings.mean(dim=0), context_emb])
        else:
            combined_input = modality_embeddings.mean(dim=0)
        
        weights = self.modality_importance_predictor(combined_input)
        
        # 加权融合
        weighted_output = sum(
            weight * emb for weight, emb in zip(weights, modality_embeddings.values())
        )
        
        return weighted_output, weights
```

**关键技术挑战**：

1. **模态对齐**：
```python
class ModalityAlignment(nn.Module):
    def __init__(self):
        self.alignment_networks = nn.ModuleDict({
            'text_image': ContrastiveLearning(),
            'text_audio': ContrastiveLearning(),
            'image_audio': ContrastiveLearning()
        })
    
    def align_modalities(self, modality_pairs):
        alignment_losses = {}
        
        for pair_name, (mod1, mod2) in modality_pairs.items():
            # 对比学习对齐
            alignment_loss = self.alignment_networks[pair_name](
                mod1, mod2
            )
            alignment_losses[pair_name] = alignment_loss
        
        return alignment_losses
```

2. **时序同步**：
```python
class TemporalSynchronization(nn.Module):
    def __init__(self):
        self.temporal_aligner = TemporalAttention()
        self.sync_predictor = nn.Linear(hidden_dim, 1)
    
    def synchronize(self, video_frames, audio_segments, text_timestamps):
        # 预测最佳同步点
        sync_scores = self.sync_predictor(
            torch.cat([video_frames, audio_segments], dim=-1)
        )
        
        # 基于同步分数对齐
        aligned_features = self.temporal_aligner(
            video_frames, audio_segments, sync_scores
        )
        
        return aligned_features
```

3. **可控生成**：
```python
class ControllableMultiModalGeneration(nn.Module):
    def __init__(self):
        self.style_controller = StyleController()
        self.content_controller = ContentController()
        self.modality_controller = ModalityController()
    
    def controlled_generate(self, input_modalities, control_signals):
        # 解析控制信号
        style_control = control_signals.get('style', None)
        content_control = control_signals.get('content', None)
        modality_control = control_signals.get('output_modality', 'text')
        
        # 编码输入
        encoded_input = self.encoder(input_modalities)
        
        # 应用控制
        if style_control:
            encoded_input = self.style_controller(encoded_input, style_control)
        
        if content_control:
            encoded_input = self.content_controller(encoded_input, content_control)
        
        # 生成指定模态输出
        output = self.modality_controller.generate(
            encoded_input, target_modality=modality_control
        )
        
        return output
```

**系统架构设计**：

```python
class IdealMultiModalEncoderDecoder(nn.Module):
    def __init__(self):
        # 编码器组件
        self.unified_encoder = UnifiedModalityEncoder()
        self.hierarchical_attention = HierarchicalMultiModalAttention()
        self.modality_aligner = ModalityAlignment()
        
        # 解码器组件
        self.controllable_decoder = ControllableMultiModalGeneration()
        self.adaptive_weighting = AdaptiveModalityWeighting()
        
        # 辅助组件
        self.temporal_sync = TemporalSynchronization()
        self.quality_assessor = QualityAssessment()
    
    def forward(self, inputs, task_type, control_signals=None):
        # 1. 多模态编码
        encoded_modalities = self.unified_encoder(inputs)
        
        # 2. 模态对齐
        alignment_losses = self.modality_aligner.align_modalities(
            self.create_modality_pairs(encoded_modalities)
        )
        
        # 3. 层次化注意力
        attended_features = self.hierarchical_attention(encoded_modalities)
        
        # 4. 自适应权重
        weighted_features, modality_weights = self.adaptive_weighting(
            attended_features, task_context=task_type
        )
        
        # 5. 可控生成
        output = self.controllable_decoder.controlled_generate(
            weighted_features, control_signals or {}
        )
        
        # 6. 质量评估
        quality_score = self.quality_assessor(output, inputs)
        
        return {
            'output': output,
            'modality_weights': modality_weights,
            'alignment_losses': alignment_losses,
            'quality_score': quality_score
        }
```

**评估框架**：

```python
class MultiModalEvaluationFramework:
    def __init__(self):
        self.modality_specific_metrics = {
            'text': ['bleu', 'rouge', 'bertscore'],
            'image': ['fid', 'inception_score', 'clip_score'],
            'audio': ['mel_distance', 'pitch_accuracy'],
            'video': ['video_quality', 'temporal_consistency']
        }
        
        self.cross_modal_metrics = [
            'semantic_consistency',
            'temporal_alignment',
            'style_consistency'
        ]
    
    def comprehensive_evaluate(self, model_outputs, ground_truth):
        results = {}
        
        # 单模态评估
        for modality in model_outputs:
            if modality in ground_truth:
                modality_results = {}
                for metric in self.modality_specific_metrics[modality]:
                    score = self.calculate_metric(
                        metric, model_outputs[modality], ground_truth[modality]
                    )
                    modality_results[metric] = score
                results[modality] = modality_results
        
        # 跨模态评估
        cross_modal_results = {}
        for metric in self.cross_modal_metrics:
            score = self.calculate_cross_modal_metric(
                metric, model_outputs, ground_truth
            )
            cross_modal_results[metric] = score
        results['cross_modal'] = cross_modal_results
        
        return results
```

**关键考虑因素总结**：

1. **技术层面**：
   - 模态统一表示和对齐
   - 高效的注意力机制
   - 可控的生成过程
   - 鲁棒的质量评估

2. **工程层面**：
   - 可扩展的架构设计
   - 高效的计算和存储
   - 模块化的组件设计
   - 完善的监控和调试

3. **应用层面**：
   - 灵活的任务适应
   - 用户友好的接口
   - 实时性能要求
   - 隐私和安全考虑

**追问**：如何处理不同模态数据的质量差异和缺失问题？

## 面试技巧与总结

### 回答策略

1. **层次化回答**：
   - 先回答核心概念
   - 再深入技术细节
   - 最后讨论应用和影响

2. **对比分析**：
   - 与其他架构对比
   - 分析优势和局限
   - 讨论适用场景

3. **实践导向**：
   - 结合具体项目经验
   - 讨论工程实现细节
   - 分享优化技巧

4. **前瞻思考**：
   - 分析技术发展趋势
   - 讨论未来可能的改进
   - 提出创新想法

### 常见陷阱

1. **理论脱离实际**：只谈论文，不谈实现
2. **细节过度**：陷入技术细节，忽略整体
3. **缺乏对比**：不能清晰区分不同方法
4. **固化思维**：不能灵活应对变化的问题

### 进阶学习建议

1. **动手实践**：
   - 微调T5/BART模型
   - 实现自定义预训练任务
   - 分析不同解码策略
   - 优化推理性能

2. **深入研究**：
   - 阅读核心论文和代码
   - 理解架构设计原理
   - 分析实验结果
   - 复现关键实验

3. **跟踪前沿**：
   - 关注最新研究进展
   - 参与开源项目
   - 交流讨论心得
   - 尝试创新改进

4. **系统思维**：
   - 理解技术发展脉络
   - 思考应用场景
   - 考虑工程实现
   - 评估商业价值

---

**记住**：Encoder-Decoder架构体现了"理解与生成"的完美结合，掌握这种架构不仅是技术能力的体现，更是对AI系统设计哲学的深刻理解。在面试中展现你的技术深度、实践经验和创新思维，才能真正脱颖而出。